setwd("~/STORAGE/tests_courses/courses/statitics_URP/Anova")
dieta1<-c(198,211,240,189,178)
dieta2<-c(214,200,259,194,188)
dieta3<-c(174,176,213,201,158)
dietas<- c(dieta1,dieta2,dieta3)
dietas
trats <- gl(3,5,label=c("dieta1","dieta2","dieta3")) #3 tratamientos, 5 valores que corresponden a la variabnle del "peso" de la dieta.
length(trats)==length(dietas)
is.numeric(trats)
is.factor(trats)
anova1 <- aov(dietas~trats)
View(anova1)
summary(anova1)
di<-data.frame(dietas, trats)
library(ggplot2)
windows()
p <- ggplot(di, aes(factor(trats), dietas))
p +scale_y_continuous(name = "Peso (libras)") + scale_x_discrete(name="Tratamientos")+ geom_boxplot()
windows()
par(mfrow=c(2,2)) #Particiona mi ventana grafica
plot(anova1) #Contrasta los supuestos del ANOVA, de forma grafica
reg<-lm(dietas~trats) #permite obtener el modelo de regresión lineal
residuos<-residuals(reg) #Estima los residuos
shapiro.test(residuos)
library(car)
install.packages("call")
leveneTest(dietas~trats)
install.packages("car")
leveneTest(dietas~trats)
library(car)
leveneTest(dietas~trats)
TukeyHSD(anova1) #Calcula la prueba de Tukey
windows()
plot(TukeyHSD(anova1)) #Genera un plot para Tukey
kruskal.test(dietas~trats)
data(InsectSprays)
attach(InsectSprays)
str(InsectSprays) #Permite ver las características de las variables
InsectSprays2 <-
InsectSprays[(InsectSprays$spray=="C"|
InsectSprays$spray=="D"|
InsectSprays$spray=="E"),]
str(InsectSprays2)
tapply(count, spray, mean)
windows()
plot(TukeyHSD(anova1)) #Genera un plot para Tukey
TukeyHSD(andeva3)
tapply(count, spray, mean) #Permite ver la media de los factores (Tratamientos)
tapply(count, spray, sd) #Permite ver la desviación estándar de los factores
tapply(count, spray, length) #Permite ver la cantidad de datos.
library(ggplot2)
boxplot(count ~ spray, data=InsectSprays2, col="16", xlab="Tipos de Spray", ylab="Cantidad de Insectos")
ggplot(InsectSprays,aes(spray, count, colour = spray ))+geom_point() + geom_boxplot()
TukeyHSD(anova1) #Calcula la prueba de Tukey
setwd("~/STORAGE/tests_courses/courses/statitics_URP/base_de_datos")
install.packages("caret")
library(caret)
rm(list=ls())
library(caret)
Train=read.csv("data_loan_status_limpia.csv"
sample_n(Train, 3)
cor(Train[,1:8])
Train=read.csv("data_loan_status_limpia.csv")
setwd("~/STORAGE/tests_courses/courses/statitics_URP/ArbolesClasificación")
library(caret)
Train=read.csv("data_loan_status_limpia.csv")
View(Train)
sample_n(Train, 3)
library("readxl")
library("ggplot2")
library(gee)
install.packages("gee")
library(gee)
library(readxl)
library(leaps)
install.packages("leaps")
library(leaps)
install.packages("dplyr")
install.packages("dplyr")
library(caret)
library(pROC)
install.packages("pROC")
install.packages("party")
library(party)
install.packages("sandwich")
install.packages("C50")
install.packages("rpart")
library("readxl")
library("ggplot2")
library(gee)
library(readxl)
library(leaps)
library(dplyr)
library(caret)
library(pROC)
library(party)
library(C50)
library(rpart)
Train=read.csv("data_loan_status_limpia.csv")
sample_n(Train, 3)
cor(Train[,1:8])
set.seed(123)
training.samples <- Train$Loan_Status %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- Train[training.samples, ]
test.data <- Train[-training.samples, ]
modelo1<-ctree(Loan_Status~.,data = train.data,
controls=ctree_control(mincriterion=0.95))
View(modelo1)
proba1 = sapply(predict(modelo1, newdata=test.data,type="prob"),'[[',1)
AUC1 <- roc(test.data$Loan_Status, proba1)
auc_modelo1=AUC1$auc
gini1 <- 2*(AUC1$auc) -1
PRED <-ifelse(proba1 <= 0.5 ,0,1)
tabla=confusionMatrix(PRED,test.data$Loan_Status,positive = "1")
View(train.data)
View(test.data)
install.packages("e1071")
Train=read.csv("data_loan_status_limpia.csv")
Train$Loan_Status = as.factor()
Train=read.csv("data_loan_status_limpia.csv")
Train$Loan_Status = as.factor(Train$Loan_Status)
sample_n(Train, 3)
cor(Train[,1:8])
cor(Train[,1:8])
Train=read.csv("data_loan_status_limpia.csv")
Train$Loan_Status = as.factor(Train$Loan_Status)
sample_n(Train, 3)
set.seed(123)
training.samples <- Train$Loan_Status %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- Train[training.samples, ]
test.data <- Train[-training.samples, ]
modelo1<-ctree(Loan_Status~.,data = train.data,
controls=ctree_control(mincriterion=0.95))
proba1 = sapply(predict(modelo1, newdata=test.data,type="prob"),'[[',1)
AUC1 <- roc(test.data$Loan_Status, proba1)
auc_modelo1=AUC1$auc
gini1 <- 2*(AUC1$auc) -1
PRED <-ifelse(proba1 <= 0.5 ,0,1)
PRED <-ifelse(proba1 <= 0.5 ,0,1)
PRED = as.factor(PRED)
tabla=confusionMatrix(PRED,test.data$Loan_Status,positive = "1")
# sensibilidad
Sensitivity1=as.numeric(tabla$byClass[1])
# Precision
Accuracy1=tabla$overall[1]
# Calcular el error de mala clasificaci?n
error1=mean(PRED!=test.data$Loan_Status)
# indicadores
auc_modelo1
gini1
Accuracy1
error1
Sensitivity1
arbol.completo <- rpart(Loan_Status~.,data = train.data,method="class",cp=0, minbucket=0)
xerr <- arbol.completo$cptable[,"xerror"] ## error de la validacion cruzada
minxerr <- which.min(xerr)
mincp <- arbol.completo$cptable[minxerr, "CP"]
modelo2 <- prune(arbol.completo,cp=mincp)
rm(list=ls())
princomp(dataset)
Europa=read.csv("http://www.instantr.com/wp-content/uploads/2013/01/europe.csv", header=TRUE)
head(Europa, 3)
Europa2=Europa[,c(2,3,4,5,6)]
View(Europa2)
head(Europa2, 3)
View(Europa2)
summary(pca.Europa2)
pca.Europa2 <- prcomp(Europa[,2:8], scale=FALSE)
summary(pca.Europa2)
plot(pca.Europa2)
plot(pca.Europa2)
correlacion <- cor(Europa2)
View(correlacion)
pca.Europa2 <- prcomp(Europa[,2:8], scale=FALSE)
View(pca.Europa2)
summary(pca.Europa2)
pca.Europa2 <- prcomp(Europa[,2:8], scale=TRUE)
summary(pca.Europa2)
plot(pca.Europa2)
pca.Europa2 <- prcomp(Europa2, scale=T)
summary(pca.Europa2)
plot(pca.Europa2)
pca.Europa2$sdev # Criterio de Kasiser
pca.Europa2$rotation # cargas de cada componente.
head(pca.Europa2$x, 3)
pca.Europa2$sdev # Autovalores de los Componentes
pca.Europa2$rotation
pca.Europa2$x # Puntuaciones factoriales
Componentes<-pca.Europa2$x[,1:3]
Europa3<-cbind(Europa,Componentes)
View(Europa3)
library(FactoMineR)
install.packages("FactoMineR")
library("factoextra")
library("FactoMineR")
install.packages("factoextra")
library("factoextra")
library("FactoMineR")
data("housetasks")
res.ca <- CA(housetasks, graph = FALSE)
get_ca_row(res.ca)
get_ca_col(res.ca)
windows()
fviz_contrib(res.ca, choice = "row", axes = 2)
windows()
fviz_contrib(res.ca, choice = "col", axes = 1)
windows()
fviz_ca_row(res.ca, repel = TRUE)
windows()
fviz_ca_col(res.ca)
windows()
fviz_ca_biplot(res.ca, repel = TRUE)
data(poison)
res.mca <- MCA(poison, quanti.sup = 1:2,
quali.sup = 3:4, graph=FALSE)
get_mca_var(res.mca)
get_mca_ind(res.mca)
fviz_contrib(res.mca, choice ="var", axes = 1)
fviz_contrib(res.mca, choice ="ind", axes = 1, top = 20)
grp <- as.factor(poison[, "Vomiting"])
fviz_mca_ind(res.mca,  habillage = grp,
addEllipses = TRUE, repel = TRUE)
fviz_mca_var(res.mca, repel = TRUE)
library("cluster")
library("fpc")
library("mclust")
library("dbscan")
library("readxl")
install.packages("dbscan")
install.packages("cluster")
install.packages("fpc")
install.packages("mclust")
setwd("~/STORAGE/tests_courses/courses/statitics_URP/base_de_datos")
install.packages("readxl")
carros<-read.csv("BaseAutos.csv", header=TRUE, sep=",", dec=".")
numericos <- sapply(carros, is.numeric) # Le aplicamos el
carros<-read.csv("carros2011imputado2.csv", header=TRUE, sep=",", dec=".")
carros<-read.csv("carros2011imputado2.csv", header=TRUE, sep=",", dec=".")
carros<-read.csv("carros2011imputado2.csv", header=TRUE, sep=",", dec=".")
carros<-read.csv("carros2011imputado2.csv", header=TRUE, sep=",", dec=".")
numericos <- sapply(carros, is.numeric) # Le aplicamos el
carrosnum<- carros[ , numericos]
carroslabel<-paste(carros$fabricante,carros$modelo)
carrosnormal<-scale(carrosnum[,-1])
medias<-attr(carrosnormal,"scaled:center")
desviaciones<-attr(carrosnormal,"scaled:scale")
ejemploeuclid <- dist(carrosnormal[2:3,], method="euclidean")
carrosdist<-dist(carrosnormal, method="euclidean")
set.seed(100) # Semilla aleatoria
carroskmcluster<-kmeans(carrosnormal, centers=3, iter.max=100)
carroskmcluster$iter # Ver numero de iteraciones
# Cluster de pertenencia
carroskmcluster$cluster
# Centroides o centros de gravedad
carroskmcluster$centers
carroskmcluster$size
par(mfrow=c(1,1))
clusplot(carros,carroskmcluster$cluster, color=TRUE)
install.packages("clusplot")
clusplot(carros,carroskmcluster$cluster, color=TRUE)
library("abind", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
library("assertthat", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
library("backports", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
library("BBmisc", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
library("beanplot", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
library("BH", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
library("bindr", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
library("bindrcpp", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
library("bitops", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
library("C50", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
library("car", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
clusplot(carros,carroskmcluster$cluster, color=TRUE)
install.packages("clusplot")
install.packages("ggplot2")
clusplot(carros,carroskmcluster$cluster, color=TRUE)
install.packages("plot3D")
library(ggplot2)
clusplot(carros,carroskmcluster$cluster, color=TRUE)
library("cluster")
library("fpc")
library("mclust")
library("dbscan")
library("readxl")
carros<-read.csv("carros2011imputado2.csv", header=TRUE, sep=",", dec=".")
numericos <- sapply(carros, is.numeric) # Le aplicamos el
# argumento2 a el argumento1.
# Me quedo solo con las numericas
carrosnum<- carros[ , numericos]
# Necesitamos las variables de identificacion
carroslabel<-paste(carros$fabricante,carros$modelo)
# Estandarizamos
carrosnormal<-scale(carrosnum[,-1])  # retiramos el ID
# Conocer/guardar las medias y desviaciones
medias<-attr(carrosnormal,"scaled:center")
desviaciones<-attr(carrosnormal,"scaled:scale")
# Metricas de distancia: usar dist() o libreria philentropy
ejemploeuclid <- dist(carrosnormal[2:3,], method="euclidean")
carrosdist<-dist(carrosnormal, method="euclidean")
# Aplicamos el cluster no jerarquico KMeans
set.seed(100) # Semilla aleatoria
carroskmcluster<-kmeans(carrosnormal, centers=3, iter.max=100)
carroskmcluster$iter # Ver numero de iteraciones
# Cluster de pertenencia
carroskmcluster$cluster
# Centroides o centros de gravedad
carroskmcluster$centers
# Tamaño de los clusters
carroskmcluster$size
par(mfrow=c(1,1))
library(ggplot2)
clusplot(carros,carroskmcluster$cluster, color=TRUE)
# Calcula la suma total de cuadrados
wss <- (nrow(carrosnormal)-1)*sum(apply(carrosnormal,2,var))
# La calcula por clusters
for (i in 2:15) wss[i] <- sum(kmeans(carrosnormal,
centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Nummero de Clusters",
ylab="Suma de cuadrados within")
# Evaluar usando el criterio CH Indice Calinski Harabasz
#es un indice basado/cercano a una F de anova
set.seed(26935)
clustering.ch <- kmeansruns(carrosnormal,krange=2:60,
criterion="asw",iter.max=100,
runs= 100,critout=TRUE)
clustering.ch$bestk
# Evaluar usando el criterio ASW (average sillouethe width)
set.seed(2) #Para evitar aleatoriedad en los resultados
clustering.asw <- kmeansruns(carrosnormal,krange=2:10,criterion="asw",iter.max=100, runs= 100,critout=TRUE)
clustering.asw$bestk
clustering.asw$crit
gscar<-clusGap(carrosnormal,FUN=kmeans,K.max=20,B=60)
gscar
#validar resultados- consistencia
kclusters <- clusterboot(carrosnormal,B=10,clustermethod=kmeansCBI,k=5,seed=5)
#la validacion del resultado. <0.75 o .85 muy bueno; <.6 malo
kclusters$bootmean
#el grupo de pertenencia
hgroups <- kclusters$result$partition
set.seed(2) #Para evitar aleatoriedad en los resultados
clustering.asw <- kmeansruns(carrosnormal,krange=2:10,criterion="asw",iter.max=100, runs= 100,critout=TRUE)
clustering.asw$bestk
clustering.asw$crit
set.seed(26935)
clustering.ch <- kmeansruns(carrosnormal,krange=2:60,
criterion="asw",iter.max=100,
runs= 100,critout=TRUE)
clustering.ch$bestk
gscar<-clusGap(carrosnormal,FUN=kmeans,K.max=20,B=60)
gscar
kclusters <- clusterboot(carrosnormal,B=10,clustermethod=kmeansCBI,k=5,seed=5)
#la validacion del resultado. <0.75 o .85 muy bueno; <.6 malo
kclusters$bootmean
#el grupo de pertenencia
hgroups <- kclusters$result$partition
