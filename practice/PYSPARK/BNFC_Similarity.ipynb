{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark \n",
    "findspark.init()\n",
    "\n",
    "from pyspark.ml.feature import MinHashLSH, BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import coalesce, udf, struct, col, lit, unix_timestamp, count, when, isnan, isnull, split\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from IPython.display import display\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, MinMaxScaler\n",
    "from pyspark.ml.clustering import KMeans, GaussianMixture, BisectingKMeans\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "spark = SparkSession.builder.appName('laptop_everis').getOrCreate()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1ER METODO USANDO KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262144 256\n",
      "236195 256 25949 256\n"
     ]
    }
   ],
   "source": [
    "SEED = 29082013\n",
    "\n",
    "df = spark.read.csv(\"train.csv\", header=True, nullValue=\"?\", inferSchema=True)\n",
    "train, test = df.randomSplit([0.9, 0.1], seed=SEED)\n",
    "#ds.printSchema()\n",
    "\n",
    "# SELECT COLUMNS\n",
    "cols_features = df.columns[1:-1]\n",
    "print(df.count(), len(cols_features))\n",
    "del df\n",
    "print(train.count(), len(train.columns[1:-1]), test.count(), len(test.columns[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_scaler_df(cols_features, ds, scaled=True):\n",
    "    \"\"\"\n",
    "        Vectorized and Scaled features \n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=cols_features, outputCol=\"features\")\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "    \n",
    "    stages = [assembler]\n",
    "    if scaled:\n",
    "        stages.append(scaler)\n",
    "    \n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    _proccess = pipeline.fit(ds)\n",
    "    \n",
    "    ds = _proccess.transform(ds).drop(*cols_features)\n",
    "    if scaled:\n",
    "        ds = ds.drop(\"features\")\n",
    "        \n",
    "    display(ds.show(5))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+\n",
      "|                  id|target|            features|\n",
      "+--------------------+------+--------------------+\n",
      "|00006042e379fc155...|     0|[-0.043731,-0.009...|\n",
      "|00034f0082232ad8f...|     1|[2.749005,0.24356...|\n",
      "|00036a71992c149e9...|     1|[2.317103,-4.3215...|\n",
      "|0007233584b5a85b4...|     1|[-9.570656,-5.163...|\n",
      "|000863c7e31c5f2c6...|     1|[1.436977,0.63941...|\n",
      "+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+\n",
      "|                  id|target|            features|\n",
      "+--------------------+------+--------------------+\n",
      "|0008851be01140e75...|     0|[1.633524,5.21490...|\n",
      "|00161f97cdda20b20...|     0|[0.810057,0.28687...|\n",
      "|002973cbedd83d830...|     1|[0.07062,5.837957...|\n",
      "|002bde7fd91b49aaa...|     0|[1.146199,-0.0767...|\n",
      "|003cb39fb6c0a2556...|     1|[1.542351,0.08234...|\n",
      "+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = vector_scaler_df(\n",
    "    cols_features, train, scaled=False\n",
    ")\n",
    "test = vector_scaler_df(\n",
    "    cols_features, test, scaled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSE ENTRENA CON LOS DATOS DE ENTRENAMIENTO PARA LOGRAR OBTENER LOS CENTROIDES DE ESTOS MISMOS\n",
    "clusters = 3\n",
    "max_iter = 100\n",
    "\n",
    "# TRAIN AND PREDICT\n",
    "kmeans_clf = BisectingKMeans()\\\n",
    "         .setK(clusters)\\\n",
    "         .setMaxIter(max_iter)\\\n",
    "         .setSeed(SEED)\\\n",
    "         .setFeaturesCol(\"features\")\\\n",
    "         .setPredictionCol(\"cluster\")\\\n",
    "         .setDistanceMeasure(\"euclidean\")\n",
    "\n",
    "model_km = kmeans_clf.fit(train)\n",
    "print(dir(model_km))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(kmeans_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_km.transform(train).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_with_cluster = model_km.transform(test)\n",
    "test_with_cluster.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIPOS DE DATO DE LOS CENTROIDES\n",
    "\n",
    "for center in model_km.clusterCenters():\n",
    "    print(type(center))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los centroides.\n",
    "import numpy as np\n",
    "\n",
    "centers = [\n",
    "    list([float(num) for num in _]) for _ in np.round(model_km.clusterCenters(), 6)\n",
    "]\n",
    "print(type(centers))\n",
    "\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(type(center), len(center), center[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from scipy.spatial.distance import jaccard, cosine, euclidean\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "from scipy.spatial import distance\n",
    "\n",
    "order = 0\n",
    "\n",
    "for center in centers:\n",
    "    distance_udf = F.udf(lambda x: float(euclidean(x, center)), FloatType())\n",
    "    cluster_train = cluster_train.withColumn('distance_centroid_{}'.format(order), distance_udf(F.col('scaled_features')))\n",
    "    order += 1\n",
    "\n",
    "cluster_train.show(15)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "order = 0\n",
    "\n",
    "for center in centers:\n",
    "    distance_udf = F.udf(lambda x: float(euclidean(x, center)), FloatType())\n",
    "    cluster_test = cluster_test.withColumn('distance_centroid_{}'.format(order), distance_udf(F.col('scaled_features')))\n",
    "    order += 1\n",
    "\n",
    "cluster_test.show(15)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, FloatType, DoubleType\n",
    "\n",
    "df_centers = sc.parallelize(centers).toDF(cols_features)\n",
    "df_centers = vector_scaler_df(cols_features, df_centers, scaled=False)\n",
    "print(type(df_centers))\n",
    "df_centers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_features = train.select('id', 'features')\n",
    "only_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for row in only_features.take(5):\n",
    "    print(\"----- ------------------\", type(row))\n",
    "    for vect in row:\n",
    "        print(\"///// \", type(vect))\n",
    "        for val in vect:\n",
    "            print(type(val))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from pyspark.sql import types\n",
    "only_features = only_features.withColumn(\n",
    "    \"features_x\", only_features[\"features\"].cast(\n",
    "        types.ArrayType(\n",
    "            types.DoubleType()\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_features.schema\n",
    "#StructType(List(StructField(features,VectorUDT,true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(BucketedRandomProjectionLSH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brp2 = BucketedRandomProjectionLSH(\n",
    "    inputCol=\"features\", outputCol=\"hashes\", bucketLength=1\n",
    ")\n",
    "model_lsh = brp2.fit(df_centers)\n",
    "print(dir(model_lsh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_lsh.transform(only_features).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model_lsh.approxNearestNeighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "order = 0\n",
    "\n",
    "for center in model_km.clusterCenters():\n",
    "    \n",
    "    print(type(center), center.size, center[:10])\n",
    "    _result = model_lsh.approxNearestNeighbors(\n",
    "        only_features, Vectors.dense(center), 1\n",
    "    )\n",
    "    _result = _result.withColumn(\"cluster\", lit(order))\n",
    "    df_result = _result if order <= 0 else df_result.union(_result)\n",
    "    \n",
    "    order += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_result.select('cluster', col(\"id\").alias(\"id_representant\"))\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_with_cluster = test_with_cluster.join(\n",
    "    df_result, test_with_cluster.cluster == df_result.cluster, how='left'\n",
    ")\n",
    "test_with_cluster.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_with_cluster.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
