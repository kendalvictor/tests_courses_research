{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark \n",
    "findspark.init()\n",
    "\n",
    "from IPython.display import display\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "spark = SparkSession.builder.appName('laptop_everis').getOrCreate()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, MinMaxScaler\n",
    "from pyspark.ml.clustering import KMeans, GaussianMixture, BisectingKMeans\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262144 256\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+------+--------------------+\n",
      "|                  id|target|            features|\n",
      "+--------------------+------+--------------------+\n",
      "|707b395ecdcbb4dc2...|     0|[-2.070654,1.0181...|\n",
      "+--------------------+------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+------+--------------------+\n",
      "|                  id|target|     scaled_features|\n",
      "+--------------------+------+--------------------+\n",
      "|707b395ecdcbb4dc2...|     0|[0.43070225358398...|\n",
      "+--------------------+------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "ds = spark.read.csv(\"train.csv\", header=True, nullValue=\"?\", inferSchema=True)\n",
    "#ds.printSchema()\n",
    "\n",
    "# SELECT COLUMNS\n",
    "cols_features = ds.columns[1:-1]\n",
    "print(ds.count(), len(cols_features))\n",
    "\n",
    "#VECTORIZER FEATURES\n",
    "assembler = VectorAssembler(inputCols=cols_features, outputCol=\"features\")\n",
    "ds = assembler.transform(ds).drop(*cols_features)\n",
    "print(type(ds))\n",
    "display(ds.show(1))\n",
    "\n",
    "# SCALED FEATURES\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "ds = scaler.fit(ds).transform(ds).drop(\"features\")\n",
    "print(type(ds))\n",
    "display(ds.show(1))\n",
    "\n",
    "# DECLARE PARAMETERS\n",
    "clusters = 3\n",
    "max_iter = 500\n",
    "\n",
    "# TRAIN AND PREDICT\n",
    "kmeans_clf = GaussianMixture()\\\n",
    "         .setK(clusters)\\\n",
    "         .setMaxIter(max_iter)\\\n",
    "         .setFeaturesCol(\"scaled_features\")\\\n",
    "         .setPredictionCol(\"cluster\")\n",
    "\n",
    "model_gauss = kmeans_clf.fit(ds)\n",
    "print(dir(model_gauss))\n",
    "\n",
    "predict_gauss = model_gauss.transform(ds)\n",
    "display(predict_gauss.show(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BISECTING-KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262144 256\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+------+--------------------+\n",
      "|                  id|target|            features|\n",
      "+--------------------+------+--------------------+\n",
      "|707b395ecdcbb4dc2...|     0|[-2.070654,1.0181...|\n",
      "+--------------------+------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+------+--------------------+\n",
      "|                  id|target|     scaled_features|\n",
      "+--------------------+------+--------------------+\n",
      "|707b395ecdcbb4dc2...|     0|[0.43070225358398...|\n",
      "+--------------------+------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "ds = spark.read.csv(\"train.csv\", header=True, nullValue=\"?\", inferSchema=True)\n",
    "#ds.printSchema()\n",
    "\n",
    "# SELECT COLUMNS\n",
    "cols_features = ds.columns[1:-1]\n",
    "print(ds.count(), len(cols_features))\n",
    "\n",
    "#VECTORIZER FEATURES\n",
    "assembler = VectorAssembler(inputCols=cols_features, outputCol=\"features\")\n",
    "ds = assembler.transform(ds).drop(*cols_features)\n",
    "print(type(ds))\n",
    "display(ds.show(1))\n",
    "\n",
    "# SCALED FEATURES\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "ds = scaler.fit(ds).transform(ds).drop(\"features\")\n",
    "print(type(ds))\n",
    "display(ds.show(1))\n",
    "\n",
    "# DECLARE PARAMETERS\n",
    "clusters = 3\n",
    "max_iter = 100\n",
    "\n",
    "# TRAIN AND PREDICT\n",
    "kmeans_clf = BisectingKMeans()\\\n",
    "         .setK(clusters)\\\n",
    "         .setMaxIter(max_iter)\\\n",
    "         .setFeaturesCol(\"scaled_features\")\\\n",
    "         .setPredictionCol(\"cluster\")\\\n",
    "         .setDistanceMeasure(\"euclidean\")\n",
    "\n",
    "model_km = kmeans_clf.fit(ds)\n",
    "print(dir(model_km))\n",
    "\n",
    "predict_knn = model_knn.transform(ds)\n",
    "display(predict_km.show(10))\n",
    "\n",
    "\n",
    "# Mostrar los centroides.\n",
    "centers = model_km.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "\n",
    "    \n",
    "# EVALUATION OF RESULTS\n",
    "# Obtener la suma cuadrada de errores 'SSE'\n",
    "SSE = model_km.computeCost(predict_knn)\n",
    "print (\"Suma cuadrada de errores: \" + str(SSE))\n",
    "\n",
    "# Obtener el número de elementos\n",
    "n = predict_knn.count()\n",
    "\n",
    "# Calcular el error cuadratico medio 'RMSE'\n",
    "RMSE = math.sqrt(SSE/n)\n",
    "print (\"Error cuadratico medio: \" + str(RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = spark.read.csv(\"train.csv\", header=True, nullValue=\"?\", inferSchema=True)\n",
    "#ds.printSchema()\n",
    "\n",
    "# SELECT COLUMNS\n",
    "cols_features = ds.columns[1:-1]\n",
    "print(ds.count(), len(cols_features))\n",
    "\n",
    "#VECTORIZER FEATURES\n",
    "assembler = VectorAssembler(inputCols=cols_features, outputCol=\"features\")\n",
    "ds = assembler.transform(ds).drop(*cols_features)\n",
    "print(type(ds))\n",
    "display(ds.show(1))\n",
    "\n",
    "# SCALED FEATURES\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "ds = scaler.fit(ds).transform(ds).drop(\"features\")\n",
    "print(type(ds))\n",
    "display(ds.show(1))\n",
    "\n",
    "# DECLARE PARAMETERS\n",
    "clusters = 3\n",
    "max_iter = 500\n",
    "\n",
    "# TRAIN AND PREDICT\n",
    "kmeans_clf = KMeans()\\\n",
    "         .setK(clusters)\\\n",
    "         .setMaxIter(max_iter)\\\n",
    "         .setFeaturesCol(\"scaled_features\")\\\n",
    "         .setPredictionCol(\"cluster\")\\\n",
    "         .setDistanceMeasure(\"cosine\")\n",
    "\n",
    "model_km = kmeans_clf.fit(ds)\n",
    "print(dir(model_km))\n",
    "\n",
    "predict_knn = model_knn.transform(ds)\n",
    "display(predict_km.show(10))\n",
    "\n",
    "\n",
    "# Mostrar los centroides.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "\n",
    "    \n",
    "# EVALUATION OF RESULTS\n",
    "# Obtener la suma cuadrada de errores 'SSE'\n",
    "SSE = model_km.computeCost(predict_knn)\n",
    "print (\"Suma cuadrada de errores: \" + str(SSE))\n",
    "\n",
    "# Obtener el número de elementos\n",
    "n = predict_knn.count()\n",
    "\n",
    "# Calcular el error cuadratico medio 'RMSE'\n",
    "RMSE = math.sqrt(SSE/n)\n",
    "print (\"Error cuadratico medio: \" + str(RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "ds = spark.read.csv(\"train.csv\", header=True, nullValue=\"?\", inferSchema=True)\n",
    "#ds.printSchema()\n",
    "\n",
    "# SELECT COLUMNS\n",
    "cols_features = ds.columns[1:-1]\n",
    "print(ds.count(), len(cols_features))\n",
    "\n",
    "#VECTORIZER FEATURES\n",
    "assembler = VectorAssembler(inputCols=cols_features, outputCol=\"features\")\n",
    "ds = assembler.transform(ds).drop(*cols_features)\n",
    "print(type(ds))\n",
    "display(ds.show(1))\n",
    "\n",
    "# SCALED FEATURES\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "ds = scaler.fit(ds).transform(ds).drop(\"features\")\n",
    "print(type(ds))\n",
    "display(ds.show(1))\n",
    "\n",
    "# DECLARE PARAMETERS\n",
    "clusters = 3\n",
    "max_iter = 500\n",
    "\n",
    "# TRAIN AND PREDICT\n",
    "kmeans_clf = BisectingKMeans()\\\n",
    "         .setK(clusters)\\\n",
    "         .setMaxIter(max_iter)\\\n",
    "         .setFeaturesCol(\"scaled_features\")\\\n",
    "         .setPredictionCol(\"cluster\")\\\n",
    "         .setDistanceMeasure(\"cosine\")\n",
    "\n",
    "model_km = kmeans_clf.fit(ds)\n",
    "print(dir(model_km))\n",
    "\n",
    "predict_knn = model_knn.transform(ds)\n",
    "display(predict_km.show(10))\n",
    "\n",
    "\n",
    "# Mostrar los centroides.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "\n",
    "    \n",
    "# EVALUATION OF RESULTS\n",
    "# Obtener la suma cuadrada de errores 'SSE'\n",
    "SSE = model_km.computeCost(predict_knn)\n",
    "print (\"Suma cuadrada de errores: \" + str(SSE))\n",
    "\n",
    "# Obtener el número de elementos\n",
    "n = predict_knn.count()\n",
    "\n",
    "# Calcular el error cuadratico medio 'RMSE'\n",
    "RMSE = math.sqrt(SSE/n)\n",
    "print (\"Error cuadratico medio: \" + str(RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = spark.read.csv(\"train.csv\", header=True, nullValue=\"?\", inferSchema=True)\n",
    "#ds.printSchema()\n",
    "\n",
    "cols_features = ds.columns[1:-1]\n",
    "print(ds.count(), len(cols_features))\n",
    "\n",
    "assembler = VectorAssembler(inputCols=cols_features, outputCol=\"features\")\n",
    "ds = assembler.transform(ds).drop(*cols_features)\n",
    "print(type(ds))\n",
    "\n",
    "display(ds.show(1))\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "ds = scaler.fit(ds).transform(ds).drop(\"features\")\n",
    "\n",
    "display(ds.show(1))\n",
    "\n",
    "SEED = 29082013\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "cost = np.zeros(20)\n",
    "for k in range(2,20):\n",
    "    kmeans = KMeans().setK(k).setSeed(SEED).setFeaturesCol(\"scaled_features\")\n",
    "    model = kmeans.fit(ds.sample(False, 0.5, seed=SEED))\n",
    "    cost[k] = model.computeCost(ds) # requires Spark 2.0 or later\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize =(14,6))\n",
    "ax.plot(range(2,20), cost[2:20])\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('cost')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
