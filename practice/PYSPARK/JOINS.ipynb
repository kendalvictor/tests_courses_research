{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.8.111:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>laptop_everis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2cf75cc8d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.ml.feature import MinHashLSH, BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import coalesce, udf, struct, col, lit, unix_timestamp, count, when, isnan, isnull, split\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from IPython.display import display\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, MinMaxScaler\n",
    "from pyspark.ml.clustering import KMeans, GaussianMixture, BisectingKMeans\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, StructType, StructField, StringType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('laptop_everis').getOrCreate()\n",
    "\n",
    "\"\"\"\n",
    "spark = SparkSession.builder\\\n",
    "       .appName(\"Simple recommendation engine using Spark MLlib\")\\\n",
    "       .config(\"spark.some.config.option\", \"config-value\")\\\n",
    "       .getOrCreate()\\\n",
    "\"\"\"\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.8.111:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>laptop_everis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=laptop_everis>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src='https://2.bp.blogspot.com/-oBPhcEuXFA0/VwpQHERiVPI/AAAAAAAAFsg/r4yUWXmXeQ0ec4YsAGp-UTBeGpvS3mUDg/s1600/LEFT%2Bvs%2BRight%2BOuter%2BJoin%2Bin%2BSQL.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "| PC1|  P2|  P3|\n",
      "+----+----+----+\n",
      "|null|   4|   C|\n",
      "|   0|   5|   C|\n",
      "|   1|   2|   A|\n",
      "|   1|null|   D|\n",
      "|   2|   1|   B|\n",
      "|   3|   1|   C|\n",
      "|   4|  11|   D|\n",
      "|   9|null|null|\n",
      "|  10|   4|null|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A = spark.createDataFrame(\n",
    "    [\n",
    "        [1,2,'A'], [2,1,'B'], [3,1,'C'], \n",
    "        [4,11,'D'],[1,None,'D'], [0, 5,'C'],\n",
    "        [None,4,'C'], [9,None , None], [10,4, None],\n",
    "    ],\n",
    "    ['PC1','P2','P3']\n",
    ").sort(\n",
    "    F.col(\"PC1\").asc()\n",
    ")\n",
    "\n",
    "A.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method join in module pyspark.sql.dataframe:\n",
      "\n",
      "join(other, on=None, how=None) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Joins with another :class:`DataFrame`, using the given join expression.\n",
      "    \n",
      "    :param other: Right side of the join\n",
      "    :param on: a string for the join column name, a list of column names,\n",
      "        a join expression (Column), or a list of Columns.\n",
      "        If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      "        the column(s) must exist on both sides, and this performs an equi-join.\n",
      "    :param how: str, default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      "        ``full``, ``full_outer``, ``left``, ``left_outer``, ``right``, ``right_outer``,\n",
      "        ``left_semi``, and ``left_anti``.\n",
      "    \n",
      "    The following performs a full outer join between ``df1`` and ``df2``.\n",
      "    \n",
      "    >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height).collect()\n",
      "    [Row(name=None, height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      "    \n",
      "    >>> df.join(df2, 'name', 'outer').select('name', 'height').collect()\n",
      "    [Row(name='Tom', height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      "    \n",
      "    >>> cond = [df.name == df3.name, df.age == df3.age]\n",
      "    >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\n",
      "    [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      "    \n",
      "    >>> df.join(df2, 'name').select(df.name, df2.height).collect()\n",
      "    [Row(name='Bob', height=85)]\n",
      "    \n",
      "    >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\n",
      "    [Row(name='Bob', age=5)]\n",
      "    \n",
      "    .. versionadded:: 1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(A.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "| PC1|  P2|  P3|\n",
      "+----+----+----+\n",
      "|null|   4|   C|\n",
      "|null|   9|   C|\n",
      "|   0|   0|   A|\n",
      "|   0|null|   D|\n",
      "|   1|   1|   B|\n",
      "|   1|   4|   C|\n",
      "|   2|   3|   D|\n",
      "|   2|   1|   C|\n",
      "|  10|null|   C|\n",
      "|  11|   4|   C|\n",
      "|  12|   4|null|\n",
      "|  13|   4|null|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "B = spark.createDataFrame(\n",
    "    [\n",
    "        [0,0,'A'], [1,1,'B'], [2,1,'C'], \n",
    "        [0,None,'D'],[2,3,'D'], [1,4,'C'],\n",
    "        [11,4,'C'], [10, None,'C'], [None,9,'C'],\n",
    "        [None, 4, 'C'], [13, 4, None], [12, 4, None]\n",
    "    ],\n",
    "    ['PC1','P2','P3']\n",
    ").sort(\n",
    "    F.col(\"PC1\").asc()\n",
    ")\n",
    "\n",
    "B.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+---+\n",
      "|PC1|  P2|  P3|  P2| P3|\n",
      "+---+----+----+----+---+\n",
      "|  0|   5|   C|   0|  A|\n",
      "|  0|   5|   C|null|  D|\n",
      "|  1|   2|   A|   1|  B|\n",
      "|  1|   2|   A|   4|  C|\n",
      "|  1|null|   D|   1|  B|\n",
      "|  1|null|   D|   4|  C|\n",
      "| 10|   4|null|null|  C|\n",
      "|  2|   1|   B|   1|  C|\n",
      "|  2|   1|   B|   3|  D|\n",
      "+---+----+----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A.join(B, on='PC1', how='inner').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEFT_SEMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "|PC1|  P2|  P3|\n",
      "+---+----+----+\n",
      "|  0|   5|   C|\n",
      "|  1|   2|   A|\n",
      "|  1|null|   D|\n",
      "| 10|   4|null|\n",
      "|  2|   1|   B|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A.join(B, on='PC1', how='left_semi').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEFT_ANTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "| PC1|  P2|  P3|\n",
      "+----+----+----+\n",
      "|null|   4|   C|\n",
      "|   9|null|null|\n",
      "|   3|   1|   C|\n",
      "|   4|  11|   D|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A.join(B, on='PC1', how='left_anti').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "| PC1|  P2|  P3|  P2|  P3|\n",
      "+----+----+----+----+----+\n",
      "|   0|   5|   C|   0|   A|\n",
      "|   0|   5|   C|null|   D|\n",
      "|null|   4|   C|null|null|\n",
      "|null|null|null|   9|   C|\n",
      "|null|null|null|   4|   C|\n",
      "|   9|null|null|null|null|\n",
      "|   1|   2|   A|   1|   B|\n",
      "|   1|   2|   A|   4|   C|\n",
      "|   1|null|   D|   1|   B|\n",
      "|   1|null|   D|   4|   C|\n",
      "|  10|   4|null|null|   C|\n",
      "|   3|   1|   C|null|null|\n",
      "|  12|null|null|   4|null|\n",
      "|  11|null|null|   4|   C|\n",
      "|   2|   1|   B|   1|   C|\n",
      "|   2|   1|   B|   3|   D|\n",
      "|   4|  11|   D|null|null|\n",
      "|  13|null|null|   4|null|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A.join(B, on='PC1', how='outer').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [coalesce(PC1#785L, PC1#801L) AS PC1#930L, P2#786L, P3#787, P2#802L, P3#803]\n",
      "+- SortMergeJoin [PC1#785L], [PC1#801L], FullOuter\n",
      "   :- *(2) Sort [PC1#785L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(PC1#785L, 200)\n",
      "   :     +- *(1) Sort [PC1#785L ASC NULLS FIRST], true, 0\n",
      "   :        +- Exchange rangepartitioning(PC1#785L ASC NULLS FIRST, 200)\n",
      "   :           +- Scan ExistingRDD[PC1#785L,P2#786L,P3#787]\n",
      "   +- *(4) Sort [PC1#801L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(PC1#801L, 200)\n",
      "         +- *(3) Sort [PC1#801L ASC NULLS FIRST], true, 0\n",
      "            +- Exchange rangepartitioning(PC1#801L ASC NULLS FIRST, 200)\n",
      "               +- Scan ExistingRDD[PC1#801L,P2#802L,P3#803]\n"
     ]
    }
   ],
   "source": [
    "A.join(B, on='PC1', how='outer').explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [coalesce(PC1#785L, PC1#801L) AS PC1#942L, P2#786L, P3#787, P2#802L, P3#803]\n",
      "+- SortMergeJoin [PC1#785L], [PC1#801L], FullOuter\n",
      "   :- *(2) Sort [PC1#785L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(PC1#785L, 200)\n",
      "   :     +- *(1) Sort [PC1#785L ASC NULLS FIRST], true, 0\n",
      "   :        +- Exchange rangepartitioning(PC1#785L ASC NULLS FIRST, 200)\n",
      "   :           +- Scan ExistingRDD[PC1#785L,P2#786L,P3#787]\n",
      "   +- *(4) Sort [PC1#801L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(PC1#801L, 200)\n",
      "         +- *(3) Sort [PC1#801L ASC NULLS FIRST], true, 0\n",
      "            +- Exchange rangepartitioning(PC1#801L ASC NULLS FIRST, 200)\n",
      "               +- Scan ExistingRDD[PC1#801L,P2#802L,P3#803]\n"
     ]
    }
   ],
   "source": [
    "A.join(B, on='PC1', how='full_outer').explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "| PC1|  P2|  P3|  P2|  P3|\n",
      "+----+----+----+----+----+\n",
      "|   0|   5|   C|   0|   A|\n",
      "|   0|   5|   C|null|   D|\n",
      "|null|   4|   C|null|null|\n",
      "|null|null|null|   9|   C|\n",
      "|null|null|null|   4|   C|\n",
      "|   9|null|null|null|null|\n",
      "|   1|   2|   A|   1|   B|\n",
      "|   1|   2|   A|   4|   C|\n",
      "|   1|null|   D|   1|   B|\n",
      "|   1|null|   D|   4|   C|\n",
      "|  10|   4|null|null|   C|\n",
      "|   3|   1|   C|null|null|\n",
      "|  12|null|null|   4|null|\n",
      "|  11|null|null|   4|   C|\n",
      "|   2|   1|   B|   1|   C|\n",
      "|   2|   1|   B|   3|   D|\n",
      "|   4|  11|   D|null|null|\n",
      "|  13|null|null|   4|null|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A.join(B, on='PC1', how='full').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "| PC1|  P2|  P3|  P2|  P3|\n",
      "+----+----+----+----+----+\n",
      "|   0|   5|   C|   0|   A|\n",
      "|   0|   5|   C|null|   D|\n",
      "|null|   4|   C|null|null|\n",
      "|null|null|null|   9|   C|\n",
      "|null|null|null|   4|   C|\n",
      "|   9|null|null|null|null|\n",
      "|   1|   2|   A|   1|   B|\n",
      "|   1|   2|   A|   4|   C|\n",
      "|   1|null|   D|   1|   B|\n",
      "|   1|null|   D|   4|   C|\n",
      "|  10|   4|null|null|   C|\n",
      "|   3|   1|   C|null|null|\n",
      "|  12|null|null|   4|null|\n",
      "|  11|null|null|   4|   C|\n",
      "|   2|   1|   B|   1|   C|\n",
      "|   2|   1|   B|   3|   D|\n",
      "|   4|  11|   D|null|null|\n",
      "|  13|null|null|   4|null|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A.join(B, on='PC1', how='full_outer').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+----+----+\n",
      "| PC1| P2| P3|  P2|  P3|\n",
      "+----+---+---+----+----+\n",
      "|   0|  5|  C|   0|   A|\n",
      "|   0|  5|  C|  11|   D|\n",
      "|null|  4|  C|null|null|\n",
      "|   9|  4|  C|null|null|\n",
      "|   1|  2|  A|   1|   B|\n",
      "|   1|  2|  A|   4|   C|\n",
      "|   1|  3|  D|   1|   B|\n",
      "|   1|  3|  D|   4|   C|\n",
      "|  10|  4|  C|   4|   C|\n",
      "|   3|  1|  C|null|null|\n",
      "|   2|  1|  B|   1|   C|\n",
      "|   2|  1|  B|   3|   D|\n",
      "|   4| 11|  D|null|null|\n",
      "+----+---+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A.join(B, on='PC1', how='left').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+----+----+\n",
      "| PC1| P2| P3|  P2|  P3|\n",
      "+----+---+---+----+----+\n",
      "|   0|  5|  C|   0|   A|\n",
      "|   0|  5|  C|  11|   D|\n",
      "|null|  4|  C|null|null|\n",
      "|   9|  4|  C|null|null|\n",
      "|   1|  2|  A|   1|   B|\n",
      "|   1|  2|  A|   4|   C|\n",
      "|   1|  3|  D|   1|   B|\n",
      "|   1|  3|  D|   4|   C|\n",
      "|  10|  4|  C|   4|   C|\n",
      "|   3|  1|  C|null|null|\n",
      "|   2|  1|  B|   1|   C|\n",
      "|   2|  1|  B|   3|   D|\n",
      "|   4| 11|  D|null|null|\n",
      "+----+---+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A.join(B, on='PC1', how='left_outer').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+---+\n",
      "| PC1|  P2|  P3| P2| P3|\n",
      "+----+----+----+---+---+\n",
      "|   0|   5|   C|  0|  A|\n",
      "|   0|   5|   C| 11|  D|\n",
      "|null|null|null|  9|  C|\n",
      "|null|null|null|  4|  C|\n",
      "|   1|   2|   A|  1|  B|\n",
      "|   1|   3|   D|  1|  B|\n",
      "|   1|   2|   A|  4|  C|\n",
      "|   1|   3|   D|  4|  C|\n",
      "|  10|   4|   C|  4|  C|\n",
      "|  12|null|null|  4|  C|\n",
      "|  11|null|null|  4|  C|\n",
      "|   2|   1|   B|  1|  C|\n",
      "|   2|   1|   B|  3|  D|\n",
      "|  13|null|null|  4|  C|\n",
      "+----+----+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A.join(B, on='PC1', how='right').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+---+\n",
      "| PC1|  P2|  P3| P2| P3|\n",
      "+----+----+----+---+---+\n",
      "|   0|   5|   C|  0|  A|\n",
      "|   0|   5|   C| 11|  D|\n",
      "|null|null|null|  9|  C|\n",
      "|null|null|null|  4|  C|\n",
      "|   1|   2|   A|  1|  B|\n",
      "|   1|   3|   D|  1|  B|\n",
      "|   1|   2|   A|  4|  C|\n",
      "|   1|   3|   D|  4|  C|\n",
      "|  10|   4|   C|  4|  C|\n",
      "|  12|null|null|  4|  C|\n",
      "|  11|null|null|  4|  C|\n",
      "|   2|   1|   B|  1|  C|\n",
      "|   2|   1|   B|  3|  D|\n",
      "|  13|null|null|  4|  C|\n",
      "+----+----+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A.join(B, on='PC1', how='right_outer').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
