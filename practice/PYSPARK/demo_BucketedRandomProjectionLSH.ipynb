{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark \n",
    "findspark.init()\n",
    "\n",
    "from pyspark.ml.feature import MinHashLSH, BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from IPython.display import display\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, MinMaxScaler\n",
    "from pyspark.ml.clustering import KMeans, GaussianMixture, BisectingKMeans\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "spark = SparkSession.builder.appName('laptop_everis').getOrCreate()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataA = [(Vectors.dense([1.0, 1.0]),),\n",
    "         (Vectors.dense([1.0, -1.0]),),\n",
    "         (Vectors.dense([-1.0, -1.0]),),\n",
    "         (Vectors.dense([-1.0, 1.0]),)]\n",
    "dfA = spark.createDataFrame(dataA, [\"features\"])\n",
    "\n",
    "dataB = [(Vectors.dense([1.0, 0.0]),),\n",
    "         (Vectors.dense([-1.0, 0.0]),),\n",
    "         (Vectors.dense([0.0, 1.0]),),\n",
    "         (Vectors.dense([0.0, -1.0]),)]\n",
    "dfB = spark.createDataFrame(dataB, [\"features\"])\n",
    "\n",
    "key = Vectors.dense([1.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+-----------+\n",
      "|   features|\n",
      "+-----------+\n",
      "|  [1.0,1.0]|\n",
      "| [1.0,-1.0]|\n",
      "|[-1.0,-1.0]|\n",
      "| [-1.0,1.0]|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(dfA))\n",
    "dfA.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('features', 'vector')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfA.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(features,VectorUDT,true)))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfA.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|   features|\n",
      "+-----------+\n",
      "|  [1.0,1.0]|\n",
      "| [1.0,-1.0]|\n",
      "|[-1.0,-1.0]|\n",
      "| [-1.0,1.0]|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfA.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- ------------------ <class 'pyspark.sql.types.Row'>\n",
      "/////  <class 'pyspark.ml.linalg.DenseVector'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "----- ------------------ <class 'pyspark.sql.types.Row'>\n",
      "/////  <class 'pyspark.ml.linalg.DenseVector'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "----- ------------------ <class 'pyspark.sql.types.Row'>\n",
      "/////  <class 'pyspark.ml.linalg.DenseVector'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "----- ------------------ <class 'pyspark.sql.types.Row'>\n",
      "/////  <class 'pyspark.ml.linalg.DenseVector'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "for row in dfA.take(5):\n",
    "    print(\"----- ------------------\", type(row))\n",
    "    for vect in row:\n",
    "        print(\"///// \", type(vect))\n",
    "        for val in vect:\n",
    "            print(type(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+----------+\n",
      "|  features|\n",
      "+----------+\n",
      "| [1.0,0.0]|\n",
      "|[-1.0,0.0]|\n",
      "| [0.0,1.0]|\n",
      "|[0.0,-1.0]|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(dfB))\n",
    "dfB.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.linalg.DenseVector'>\n",
      "['__UDT__', '__add__', '__class__', '__delattr__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__radd__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_delegate', 'array', 'dot', 'norm', 'numNonzeros', 'squared_distance', 'toArray', 'values']\n"
     ]
    }
   ],
   "source": [
    "print(type(key))\n",
    "key\n",
    "print(dir(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', '__abs__', '__add__', '__and__', '__array__', '__array_finalize__', '__array_function__', '__array_interface__', '__array_prepare__', '__array_priority__', '__array_struct__', '__array_ufunc__', '__array_wrap__', '__bool__', '__class__', '__complex__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__', '__ifloordiv__', '__ilshift__', '__imatmul__', '__imod__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__xor__', 'all', 'any', 'argmax', 'argmin', 'argpartition', 'argsort', 'astype', 'base', 'byteswap', 'choose', 'clip', 'compress', 'conj', 'conjugate', 'copy', 'ctypes', 'cumprod', 'cumsum', 'data', 'diagonal', 'dot', 'dtype', 'dump', 'dumps', 'fill', 'flags', 'flat', 'flatten', 'getfield', 'imag', 'item', 'itemset', 'itemsize', 'max', 'mean', 'min', 'nbytes', 'ndim', 'newbyteorder', 'nonzero', 'partition', 'prod', 'ptp', 'put', 'ravel', 'real', 'repeat', 'reshape', 'resize', 'round', 'searchsorted', 'setfield', 'setflags', 'shape', 'size', 'sort', 'squeeze', 'std', 'strides', 'sum', 'swapaxes', 'take', 'tobytes', 'tofile', 'tolist', 'tostring', 'trace', 'transpose', 'var', 'view']\n"
     ]
    }
   ],
   "source": [
    "print(dir(key.array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hashed dataset where hashed values are stored in the column 'hashes':\n",
      "+-----------+--------------------+\n",
      "|   features|              hashes|\n",
      "+-----------+--------------------+\n",
      "|  [1.0,1.0]|[[0.0], [-1.0], [...|\n",
      "| [1.0,-1.0]|[[-1.0], [0.0], [...|\n",
      "|[-1.0,-1.0]|[[-1.0], [0.0], [...|\n",
      "| [-1.0,1.0]|[[0.0], [-1.0], [...|\n",
      "+-----------+--------------------+\n",
      "\n",
      "Approximately searching dfA for 2 nearest neighbors of the key:\n"
     ]
    }
   ],
   "source": [
    "brp = BucketedRandomProjectionLSH(\n",
    "    inputCol=\"features\", outputCol=\"hashes\", bucketLength=2.0, numHashTables=3\n",
    ")\n",
    "model = brp.fit(dfA)\n",
    "\n",
    "# Feature Transformation\n",
    "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "model.transform(dfA).show()\n",
    "\n",
    "# Compute the locality sensitive hashes for the input rows, then perform approximate\n",
    "# similarity join.\n",
    "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "# `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`\n",
    "#print(\"Approximately joining dfA and dfB on Euclidean distance smaller than 1.5:\")\n",
    "#model.approxSimilarityJoin(dfA, dfB, 1.5, distCol=\"EuclideanDistance\")\\\n",
    "#    .select(col(\"datasetA.id\").alias(\"idA\"),\n",
    "#            col(\"datasetB.id\").alias(\"idB\"),\n",
    "#            col(\"EuclideanDistance\")).show()\n",
    "\n",
    "# Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n",
    "# neighbor search.\n",
    "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "# `model.approxNearestNeighbors(transformedA, key, 2)`\n",
    "print(\"Approximately searching dfA for 2 nearest neighbors of the key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+\n",
      "|  features|              hashes|distCol|\n",
      "+----------+--------------------+-------+\n",
      "| [1.0,1.0]|[[0.0], [-1.0], [...|    1.0|\n",
      "|[1.0,-1.0]|[[-1.0], [0.0], [...|    1.0|\n",
      "+----------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.approxNearestNeighbors(dfA, key, 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $example off$\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
