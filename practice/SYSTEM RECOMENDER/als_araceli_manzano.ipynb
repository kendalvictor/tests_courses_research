{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.8.120:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>laptop_everis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe2273fd4a8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.ml.feature import MinHashLSH, BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import coalesce, udf, struct, col, lit, unix_timestamp, count, when, isnan, isnull, split\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from IPython.display import display\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, MinMaxScaler\n",
    "from pyspark.ml.clustering import KMeans, GaussianMixture, BisectingKMeans\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, StructType, StructField, StringType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('laptop_everis').getOrCreate()\n",
    "\n",
    "\"\"\"\n",
    "spark = SparkSession.builder\\\n",
    "       .appName(\"Simple recommendation engine using Spark MLlib\")\\\n",
    "       .config(\"spark.some.config.option\", \"config-value\")\\\n",
    "       .getOrCreate()\\\n",
    "\"\"\"\n",
    "spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.8.120:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>laptop_everis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=laptop_everis>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|user_id|profile_id|rating|\n",
      "+-------+----------+------+\n",
      "|      1|       133|     8|\n",
      "|      1|       720|     6|\n",
      "|      1|       971|    10|\n",
      "+-------+----------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse dating agency ratings data as a Spark dataframe\n",
    "\n",
    "ratings = \"data/ratings.dat\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), False),\n",
    "    StructField(\"profile_id\", IntegerType(), False),\n",
    "    StructField(\"rating\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "ratings_df = spark.read.format(\n",
    "    \"csv\"\n",
    ").option(\n",
    "    \"header\", \"false\"\n",
    ").option(\n",
    "    \"delimiter\", \",\"\n",
    ").schema(\n",
    "    schema\n",
    ").load(\n",
    "    ratings\n",
    ")\n",
    "\n",
    "ratings_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method drop in module pyspark.sql.dataframe:\n",
      "\n",
      "drop(how='any', thresh=None, subset=None) method of pyspark.sql.dataframe.DataFrameNaFunctions instance\n",
      "    Returns a new :class:`DataFrame` omitting rows with null values.\n",
      "    :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      "    \n",
      "    :param how: 'any' or 'all'.\n",
      "        If 'any', drop a row if it contains any nulls.\n",
      "        If 'all', drop a row only if all its values are null.\n",
      "    :param thresh: int, default None\n",
      "        If specified, drop rows that have less than `thresh` non-null values.\n",
      "        This overwrites the `how` parameter.\n",
      "    :param subset: optional list of column names to consider.\n",
      "    \n",
      "    >>> df4.na.drop().show()\n",
      "    +---+------+-----+\n",
      "    |age|height| name|\n",
      "    +---+------+-----+\n",
      "    | 10|    80|Alice|\n",
      "    +---+------+-----+\n",
      "    \n",
      "    .. versionadded:: 1.3.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ratings_df.na.drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17359346\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- profile_id: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ratings_df.count())\n",
    "ratings_df = ratings_df.na.drop(how=\"any\")\n",
    "ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_collectAsArrow', '_jcols', '_jdf', '_jmap', '_jseq', '_lazy_rdd', '_sc', '_schema', '_sort_cols', 'agg', 'alias', 'approxQuantile', 'cache', 'checkpoint', 'coalesce', 'colRegex', 'collect', 'columns', 'corr', 'count', 'cov', 'createGlobalTempView', 'createOrReplaceGlobalTempView', 'createOrReplaceTempView', 'createTempView', 'crossJoin', 'crosstab', 'cube', 'describe', 'distinct', 'drop', 'dropDuplicates', 'drop_duplicates', 'dropna', 'dtypes', 'explain', 'fillna', 'filter', 'first', 'foreach', 'foreachPartition', 'freqItems', 'groupBy', 'groupby', 'head', 'hint', 'intersect', 'isLocal', 'isStreaming', 'is_cached', 'join', 'limit', 'localCheckpoint', 'na', 'orderBy', 'persist', 'printSchema', 'randomSplit', 'rdd', 'registerTempTable', 'repartition', 'replace', 'rollup', 'sample', 'sampleBy', 'schema', 'select', 'selectExpr', 'show', 'sort', 'sortWithinPartitions', 'sql_ctx', 'stat', 'storageLevel', 'subtract', 'summary', 'take', 'toDF', 'toJSON', 'toLocalIterator', 'toPandas', 'union', 'unionAll', 'unionByName', 'unpersist', 'where', 'withColumn', 'withColumnRenamed', 'withWatermark', 'write', 'writeStream']\n"
     ]
    }
   ],
   "source": [
    "print(dir(ratings_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17359346\n"
     ]
    }
   ],
   "source": [
    "print(ratings_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|profile_id|gender|\n",
      "+----------+------+\n",
      "|         1|     F|\n",
      "|         2|     F|\n",
      "|         3|     U|\n",
      "|         4|     F|\n",
      "|         5|     F|\n",
      "|         6|     F|\n",
      "|         7|     F|\n",
      "|         8|     M|\n",
      "|         9|     M|\n",
      "|        10|     M|\n",
      "+----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gender_data = \"data/gender.dat\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"profile_id\", IntegerType(), False),\n",
    "    StructField(\"gender\", StringType(), False)\n",
    "])\n",
    "\n",
    "gender_df = spark.read.format(\n",
    "    \"csv\"\n",
    ").option(\n",
    "    \"header\", \"false\"\n",
    ").option(\n",
    "    \"delimiter\", \",\"\n",
    ").schema(\n",
    "    schema\n",
    ").load(\n",
    "    gender_data\n",
    ")\n",
    "\n",
    "gender_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_collectAsArrow', '_jcols', '_jdf', '_jmap', '_jseq', '_lazy_rdd', '_sc', '_schema', '_sort_cols', 'agg', 'alias', 'approxQuantile', 'cache', 'checkpoint', 'coalesce', 'colRegex', 'collect', 'columns', 'corr', 'count', 'cov', 'createGlobalTempView', 'createOrReplaceGlobalTempView', 'createOrReplaceTempView', 'createTempView', 'crossJoin', 'crosstab', 'cube', 'describe', 'distinct', 'drop', 'dropDuplicates', 'drop_duplicates', 'dropna', 'dtypes', 'explain', 'fillna', 'filter', 'first', 'foreach', 'foreachPartition', 'freqItems', 'groupBy', 'groupby', 'head', 'hint', 'intersect', 'isLocal', 'isStreaming', 'is_cached', 'join', 'limit', 'localCheckpoint', 'na', 'orderBy', 'persist', 'printSchema', 'randomSplit', 'rdd', 'registerTempTable', 'repartition', 'replace', 'rollup', 'sample', 'sampleBy', 'schema', 'select', 'selectExpr', 'show', 'sort', 'sortWithinPartitions', 'sql_ctx', 'stat', 'storageLevel', 'subtract', 'summary', 'take', 'toDF', 'toJSON', 'toLocalIterator', 'toPandas', 'union', 'unionAll', 'unionByName', 'unpersist', 'where', 'withColumn', 'withColumnRenamed', 'withWatermark', 'write', 'writeStream']\n"
     ]
    }
   ],
   "source": [
    "print(dir(ratings_df.select(\"rating\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method agg in module pyspark.sql.dataframe:\n",
      "\n",
      "agg(*exprs) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Aggregate on the entire :class:`DataFrame` without groups\n",
      "    (shorthand for ``df.groupBy.agg()``).\n",
      "    \n",
      "    >>> df.agg({\"age\": \"max\"}).collect()\n",
      "    [Row(max(age)=5)]\n",
      "    >>> from pyspark.sql import functions as F\n",
      "    >>> df.agg(F.min(df.age)).collect()\n",
      "    [Row(min(age)=2)]\n",
      "    \n",
      "    .. versionadded:: 1.3\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(ratings_df.select(\"rating\").agg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RATING MAXIMO\n",
    "row_max = ratings_df.agg({\"rating\": \"max\"}).collect()[0]\n",
    "max_rating = row_max[\"max(rating)\"]\n",
    "max_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RATING MINIMO\n",
    "row_min = ratings_df.agg({\"rating\": \"min\"}).collect()[0]\n",
    "min_rating = row_min[\"min(rating)\"]\n",
    "min_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc(df, col, func):\n",
    "    return df.agg(\n",
    "        {col: func}\n",
    "    ).collect()[0][\n",
    "        \"{}({})\".format(func, col)\n",
    "    ]\n",
    "\n",
    "calc(ratings_df, 'rating', 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.69,\n",
       " 1: 1.3831471805599453,\n",
       " 2: 1.7886122886681097,\n",
       " 3: 2.0762943611198903,\n",
       " 4: 2.2994379124341,\n",
       " 5: 2.481759469228055,\n",
       " 6: 2.635910149055313,\n",
       " 7: 2.7694415416798357,\n",
       " 8: 2.8872245773362195,\n",
       " 9: 2.992585092994046}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "{\n",
    "    _: math.log(_ + 1) + 0.69 for _ in range(10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------+----------+--------------+------------+\n",
      "|user_id|profile_id|rating| label|log_rating|log_rating_069|norm_log_069|\n",
      "+-------+----------+------+------+----------+--------------+------------+\n",
      "|      1|       133|     8|0.7778|    3.0794|        2.7694|      0.9031|\n",
      "|      1|       720|     6|0.5556|    2.7918|        2.4818|      0.7782|\n",
      "|      1|       971|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|      1095|     7|0.6667|    2.9459|        2.6359|      0.8451|\n",
      "|      1|      1616|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|      1978|     7|0.6667|    2.9459|        2.6359|      0.8451|\n",
      "|      1|      2145|     8|0.7778|    3.0794|        2.7694|      0.9031|\n",
      "|      1|      2211|     8|0.7778|    3.0794|        2.7694|      0.9031|\n",
      "|      1|      3751|     7|0.6667|    2.9459|        2.6359|      0.8451|\n",
      "|      1|      4062|     3|0.2222|    2.0986|        1.7886|      0.4771|\n",
      "|      1|      4633|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|      4842|     5|0.4444|    2.6094|        2.2994|      0.6989|\n",
      "|      1|      6518|     6|0.5556|    2.7918|        2.4818|      0.7782|\n",
      "|      1|      7576|     8|0.7778|    3.0794|        2.7694|      0.9031|\n",
      "|      1|      7724|     7|0.6667|    2.9459|        2.6359|      0.8451|\n",
      "|      1|      8305|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|      8923|     9|0.8889|    3.1972|        2.8872|      0.9542|\n",
      "|      1|      9345|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|      9729|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     10148|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     10811|     7|0.6667|    2.9459|        2.6359|      0.8451|\n",
      "|      1|     11671|     5|0.4444|    2.6094|        2.2994|      0.6989|\n",
      "|      1|     11747|     7|0.6667|    2.9459|        2.6359|      0.8451|\n",
      "|      1|     12638|     4|0.3333|    2.3863|        2.0763|      0.6021|\n",
      "|      1|     13210|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     13895|     5|0.4444|    2.6094|        2.2994|      0.6989|\n",
      "|      1|     14205|     1|   0.0|       1.0|          0.69|         0.0|\n",
      "|      1|     14604|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     14985|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     15123|     9|0.8889|    3.1972|        2.8872|      0.9542|\n",
      "|      1|     15408|     4|0.3333|    2.3863|        2.0763|      0.6021|\n",
      "|      1|     15530|     6|0.5556|    2.7918|        2.4818|      0.7782|\n",
      "|      1|     16480|     6|0.5556|    2.7918|        2.4818|      0.7782|\n",
      "|      1|     18032|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     18287|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     18345|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     18444|     8|0.7778|    3.0794|        2.7694|      0.9031|\n",
      "|      1|     18562|     8|0.7778|    3.0794|        2.7694|      0.9031|\n",
      "|      1|     18878|     4|0.3333|    2.3863|        2.0763|      0.6021|\n",
      "|      1|     19006|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     19231|     5|0.4444|    2.6094|        2.2994|      0.6989|\n",
      "|      1|     19727|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     20253|     6|0.5556|    2.7918|        2.4818|      0.7782|\n",
      "|      1|     20737|     8|0.7778|    3.0794|        2.7694|      0.9031|\n",
      "|      1|     21184|     6|0.5556|    2.7918|        2.4818|      0.7782|\n",
      "|      1|     21256|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     21642|     6|0.5556|    2.7918|        2.4818|      0.7782|\n",
      "|      1|     22319|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     22543|     4|0.3333|    2.3863|        2.0763|      0.6021|\n",
      "|      1|     22691|     1|   0.0|       1.0|          0.69|         0.0|\n",
      "+-------+----------+------+------+----------+--------------+------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MIN MAX SCALER\n",
    "ratings_df = ratings_df.withColumn(\n",
    "    'norm_rating', F.round((ratings_df.rating - min_rating) / (max_rating - min_rating), 4)\n",
    ").withColumn(\n",
    "    'log_rating',  F.round(F.log(ratings_df.rating) + 1, 4)\n",
    ").withColumn(\n",
    "    'log_rating_069',  F.round(F.log(ratings_df.rating) + 0.69, 4)\n",
    ")\n",
    "\n",
    "max_log_069 = calc(ratings_df, 'log_rating_069', 'max')\n",
    "min_log_069 = calc(ratings_df, 'log_rating_069', 'min')\n",
    "\n",
    "ratings_df = ratings_df.withColumn(\n",
    "    \"norm_log_069\", F.round((ratings_df.log_rating_069 - min_log_069) / (max_log_069 - min_log_069), 4)\n",
    ")\n",
    "\n",
    "df = ratings_df\\\n",
    "     .select(\"user_id\", \"profile_id\", \"rating\", \"norm_rating\", 'log_rating', 'log_rating_069', 'norm_log_069')\\\n",
    "     .withColumnRenamed(\"norm_rating\", \"label\")  # original norm_rating --> label\n",
    "\n",
    "df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntop_most_rated_profiles = ratings_df.groupBy(\"profile_id\").count().sort(F.col(\"count\").desc()).limit(50000)\\ntop_most_rated_profiles = top_most_rated_profiles.withColumnRenamed(\"profile_id\", \"popular_profile_id\")\\nprint(\"> \", top_most_rated_profiles.count())\\n\\ntop_most_rated_profiles.show(15)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 15 PERFILES MAS POPULARES\n",
    "\"\"\"\n",
    "top_most_rated_profiles = ratings_df.groupBy(\"profile_id\").count().sort(F.col(\"count\").desc()).limit(50000)\n",
    "top_most_rated_profiles = top_most_rated_profiles.withColumnRenamed(\"profile_id\", \"popular_profile_id\")\n",
    "print(\"> \", top_most_rated_profiles.count())\n",
    "\n",
    "top_most_rated_profiles.show(15)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\navg_rating_by_profile = ratings_df.groupBy(\\n    \"profile_id\"\\n).agg(\\n    F.avg(\\'rating\\').alias(\\'avg_rating\\')\\n).sort(\\n    F.col(\"avg_rating\").desc()\\n)\\nprint(\"> \", avg_rating_by_profile.count())\\n\\navg_rating_by_profile.show(50)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50 PERFILES MEJORES PUNTUADOS\n",
    "\n",
    "\"\"\"\n",
    "avg_rating_by_profile = ratings_df.groupBy(\n",
    "    \"profile_id\"\n",
    ").agg(\n",
    "    F.avg('rating').alias('avg_rating')\n",
    ").sort(\n",
    "    F.col(\"avg_rating\").desc()\n",
    ")\n",
    "print(\"> \", avg_rating_by_profile.count())\n",
    "\n",
    "avg_rating_by_profile.show(50)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntop_profiles.select(\\n    \"popular_profile_id\", \"ratio\", \"avg_rating\", \"count\"\\n).sort(\\n    F.col(\"ratio\").desc()\\n).show(25)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute a ration between rating value and popularity\n",
    "# CALCULO DE UNA RELACION ENTRE EL VALOR DE CALIFICACION Y LA POPULARIDAD\n",
    "\n",
    "\"\"\"\n",
    "top_profiles = top_most_rated_profiles.join(\n",
    "    avg_rating_by_profile, \n",
    "    top_most_rated_profiles[\"popular_profile_id\"] == avg_rating_by_profile[\"profile_id\"],\n",
    "    \"left_outer\"\n",
    ").drop(\n",
    "    'profile_id'\n",
    ").withColumn(\n",
    "    \"ratio\",\n",
    "    F.col(\"avg_rating\") / F.col(\"count\")\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Top profiles sorted by the relation average rating - number of times rated\n",
    "# 25 mejores perfiles basados en el ratio\n",
    "\n",
    "\"\"\"\n",
    "top_profiles.select(\n",
    "    \"popular_profile_id\", \"ratio\", \"avg_rating\", \"count\"\n",
    ").sort(\n",
    "    F.col(\"ratio\").desc()\n",
    ").show(25)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings_df.filter(F.col('profile_id') == 160749).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_profiles_gender = gender_df.join(top_profiles, on='profile_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'profile_id',\n",
       " 'rating',\n",
       " 'label',\n",
       " 'log_rating',\n",
       " 'log_rating_069',\n",
       " 'norm_log_069']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------+----------+--------------+------------+\n",
      "|user_id|profile_id|rating| label|log_rating|log_rating_069|norm_log_069|\n",
      "+-------+----------+------+------+----------+--------------+------------+\n",
      "|      1|       133|     8|0.7778|    3.0794|        2.7694|      0.9031|\n",
      "|      1|       720|     6|0.5556|    2.7918|        2.4818|      0.7782|\n",
      "|      1|       971|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|      1095|     7|0.6667|    2.9459|        2.6359|      0.8451|\n",
      "|      1|      1616|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|      1978|     7|0.6667|    2.9459|        2.6359|      0.8451|\n",
      "|      1|      2145|     8|0.7778|    3.0794|        2.7694|      0.9031|\n",
      "|      1|      3751|     7|0.6667|    2.9459|        2.6359|      0.8451|\n",
      "|      1|      4062|     3|0.2222|    2.0986|        1.7886|      0.4771|\n",
      "|      1|      4633|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|      4842|     5|0.4444|    2.6094|        2.2994|      0.6989|\n",
      "|      1|      6518|     6|0.5556|    2.7918|        2.4818|      0.7782|\n",
      "|      1|      7724|     7|0.6667|    2.9459|        2.6359|      0.8451|\n",
      "|      1|      8305|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|      9729|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     10148|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     10811|     7|0.6667|    2.9459|        2.6359|      0.8451|\n",
      "|      1|     11747|     7|0.6667|    2.9459|        2.6359|      0.8451|\n",
      "|      1|     12638|     4|0.3333|    2.3863|        2.0763|      0.6021|\n",
      "|      1|     13210|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "+-------+----------+------+------+----------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'profile_id',\n",
       " 'rating',\n",
       " 'label',\n",
       " 'log_rating',\n",
       " 'log_rating_069',\n",
       " 'norm_log_069']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------+----------+--------------+------------+\n",
      "|user_id|profile_id|rating| label|log_rating|log_rating_069|norm_log_069|\n",
      "+-------+----------+------+------+----------+--------------+------------+\n",
      "|      1|      2211|     8|0.7778|    3.0794|        2.7694|      0.9031|\n",
      "|      1|      7576|     8|0.7778|    3.0794|        2.7694|      0.9031|\n",
      "|      1|      8923|     9|0.8889|    3.1972|        2.8872|      0.9542|\n",
      "|      1|      9345|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     11671|     5|0.4444|    2.6094|        2.2994|      0.6989|\n",
      "|      1|     18287|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     19231|     5|0.4444|    2.6094|        2.2994|      0.6989|\n",
      "|      1|     19727|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     20737|     8|0.7778|    3.0794|        2.7694|      0.9031|\n",
      "|      1|     21256|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     21642|     6|0.5556|    2.7918|        2.4818|      0.7782|\n",
      "|      1|     22691|     1|   0.0|       1.0|          0.69|         0.0|\n",
      "|      1|     23499|     4|0.3333|    2.3863|        2.0763|      0.6021|\n",
      "|      1|     34406|    10|   1.0|    3.3026|        2.9926|         1.0|\n",
      "|      1|     35707|     1|   0.0|       1.0|          0.69|         0.0|\n",
      "|      1|     37184|     9|0.8889|    3.1972|        2.8872|      0.9542|\n",
      "|      1|     38648|     4|0.3333|    2.3863|        2.0763|      0.6021|\n",
      "|      1|     39520|     1|   0.0|       1.0|          0.69|         0.0|\n",
      "|      1|     40322|     1|   0.0|       1.0|          0.69|         0.0|\n",
      "|      1|     44877|     1|   0.0|       1.0|          0.69|         0.0|\n",
      "+-------+----------+------+------+----------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'profile_id',\n",
       " 'rating',\n",
       " 'label',\n",
       " 'log_rating',\n",
       " 'log_rating_069',\n",
       " 'norm_log_069']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "als = ALS(\n",
    "    userCol=\"user_id\", \n",
    "    itemCol=\"profile_id\", \n",
    "    ratingCol=\"label\",\n",
    "    coldStartStrategy=\"drop\", \n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Set considered parameter grid\n",
    "paramGrid = ParamGridBuilder().addGrid(\n",
    "    als.regParam, [0.01]\n",
    ").addGrid(\n",
    "    als.rank, [12]\n",
    ").addGrid(\n",
    "    als.nonnegative, [True]\n",
    ").build()\n",
    "\n",
    "# Set evaluator\n",
    "modelEvaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "\n",
    "# Set cross validator instance\n",
    "crossval = CrossValidator(estimator=als,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=modelEvaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# Perform cross-validation\n",
    "cvModel = crossval.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossValidatorModel_47cab7358acf8bd7a0d7"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__metaclass__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_clear', '_copyValues', '_copy_params', '_defaultParamMap', '_dummy', '_from_java', '_from_java_impl', '_paramMap', '_params', '_randomUID', '_resetUid', '_resolveParam', '_set', '_setDefault', '_shouldOwn', '_to_java', '_to_java_impl', '_transform', 'avgMetrics', 'bestModel', 'copy', 'estimator', 'estimatorParamMaps', 'evaluator', 'explainParam', 'explainParams', 'extractParamMap', 'getEstimator', 'getEstimatorParamMaps', 'getEvaluator', 'getOrDefault', 'getParam', 'getSeed', 'hasDefault', 'hasParam', 'isDefined', 'isSet', 'load', 'params', 'read', 'save', 'seed', 'set', 'setEstimator', 'setEstimatorParamMaps', 'setEvaluator', 'setSeed', 'transform', 'uid', 'write']\n"
     ]
    }
   ],
   "source": [
    "print(dir(cvModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CrossValidatorModel in module pyspark.ml.tuning object:\n",
      "\n",
      "class CrossValidatorModel(pyspark.ml.base.Model, ValidatorParams, pyspark.ml.util.MLReadable, pyspark.ml.util.MLWritable)\n",
      " |  CrossValidatorModel(bestModel, avgMetrics=[])\n",
      " |  \n",
      " |  CrossValidatorModel contains the model with the highest average cross-validation\n",
      " |  metric across folds and uses this model to transform input data. CrossValidatorModel\n",
      " |  also tracks the metrics for each param map evaluated.\n",
      " |  \n",
      " |  .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CrossValidatorModel\n",
      " |      pyspark.ml.base.Model\n",
      " |      pyspark.ml.base.Transformer\n",
      " |      ValidatorParams\n",
      " |      pyspark.ml.param.shared.HasSeed\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, bestModel, avgMetrics=[])\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  copy(self, extra=None)\n",
      " |      Creates a copy of this instance with a randomly generated uid\n",
      " |      and some extra params. This copies the underlying bestModel,\n",
      " |      creates a deep copy of the embedded paramMap, and\n",
      " |      copies the embedded and extra parameters over.\n",
      " |      \n",
      " |      :param extra: Extra parameters to copy to the new instance\n",
      " |      :return: Copy of this instance\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  write(self)\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  read() from builtins.type\n",
      " |      Returns an MLReader instance for this class.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.base.Model:\n",
      " |  \n",
      " |  __metaclass__ = <class 'abc.ABCMeta'>\n",
      " |      Metaclass for defining Abstract Base Classes (ABCs).\n",
      " |      \n",
      " |      Use this metaclass to create an ABC.  An ABC can be subclassed\n",
      " |      directly, and then acts as a mix-in class.  You can also register\n",
      " |      unrelated concrete classes (even built-in classes) and unrelated\n",
      " |      ABCs as 'virtual subclasses' -- these and their descendants will\n",
      " |      be considered subclasses of the registering ABC by the built-in\n",
      " |      issubclass() function, but the registering ABC won't show up in\n",
      " |      their MRO (Method Resolution Order) nor will method\n",
      " |      implementations defined by the registering ABC be callable (not\n",
      " |      even via super()).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Transformer:\n",
      " |  \n",
      " |  transform(self, dataset, params=None)\n",
      " |      Transforms the input dataset with optional parameters.\n",
      " |      \n",
      " |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      " |      :param params: an optional param map that overrides embedded params.\n",
      " |      :returns: transformed dataset\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ValidatorParams:\n",
      " |  \n",
      " |  getEstimator(self)\n",
      " |      Gets the value of estimator or its default value.\n",
      " |  \n",
      " |  getEstimatorParamMaps(self)\n",
      " |      Gets the value of estimatorParamMaps or its default value.\n",
      " |  \n",
      " |  getEvaluator(self)\n",
      " |      Gets the value of evaluator or its default value.\n",
      " |  \n",
      " |  setEstimator(self, value)\n",
      " |      Sets the value of :py:attr:`estimator`.\n",
      " |  \n",
      " |  setEstimatorParamMaps(self, value)\n",
      " |      Sets the value of :py:attr:`estimatorParamMaps`.\n",
      " |  \n",
      " |  setEvaluator(self, value)\n",
      " |      Sets the value of :py:attr:`evaluator`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from ValidatorParams:\n",
      " |  \n",
      " |  estimator = Param(parent='undefined', name='estimator', doc='estimator...\n",
      " |  \n",
      " |  estimatorParamMaps = Param(parent='undefined', name='estimatorParamMap...\n",
      " |  \n",
      " |  evaluator = Param(parent='undefined', name='evaluator', doc=...r-param...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n",
      " |  \n",
      " |  getSeed(self)\n",
      " |      Gets the value of seed or its default value.\n",
      " |  \n",
      " |  setSeed(self, value)\n",
      " |      Sets the value of :py:attr:`seed`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n",
      " |  \n",
      " |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param)\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self)\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra=None)\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      :param extra: extra param values\n",
      " |      :return: merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param)\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName)\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param)\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName)\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param)\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param)\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param, value)\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path) from builtins.type\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path)\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(cvModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALS_4c6bac31c9a3810fe610"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bet_model_als = cvModel.bestModel\n",
    "bet_model_als"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bet_model_als.rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bet_model_als._java_obj.parent().getRegParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bet_model_als._java_obj.parent().getMaxIter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bet_model_als._java_obj.parent().getNonnegative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bet_model_als._java_obj.parent().getSeed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bet_model_als._java_obj.parent().getRank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$', 'alpha', 'checkedCast', 'checkpointInterval', 'cleanShuffleDependencies$default$3', 'clear', 'coldStartStrategy', 'copy', 'copyValues', 'copyValues$default$2', 'defaultCopy', 'equals', 'explainParam', 'explainParams', 'extractParamMap', 'finalStorageLevel', 'fit', 'get', 'getAlpha', 'getCheckpointInterval', 'getClass', 'getColdStartStrategy', 'getDefault', 'getFinalStorageLevel', 'getImplicitPrefs', 'getIntermediateStorageLevel', 'getItemCol', 'getMaxIter', 'getNonnegative', 'getNumItemBlocks', 'getNumUserBlocks', 'getOrDefault', 'getParam', 'getPredictionCol', 'getRank', 'getRatingCol', 'getRegParam', 'getSeed', 'getUserCol', 'hasDefault', 'hasParam', 'hashCode', 'implicitPrefs', 'initializeLogIfNecessary', 'initializeLogIfNecessary$default$2', 'intermediateStorageLevel', 'isDefined', 'isSet', 'isTraceEnabled', 'itemCol', 'load', 'log', 'logDebug', 'logError', 'logInfo', 'logName', 'logTrace', 'logWarning', 'maxIter', 'nonnegative', 'notify', 'notifyAll', 'numItemBlocks', 'numUserBlocks', 'org$apache$spark$internal$Logging$$log_', 'org$apache$spark$internal$Logging$$log__$eq', 'org$apache$spark$ml$param$Params$$defaultParamMap', 'org$apache$spark$ml$param$Params$$paramMap', 'org$apache$spark$ml$param$Params$_setter_$org$apache$spark$ml$param$Params$$defaultParamMap_$eq', 'org$apache$spark$ml$param$Params$_setter_$org$apache$spark$ml$param$Params$$paramMap_$eq', 'org$apache$spark$ml$param$shared$HasCheckpointInterval$_setter_$checkpointInterval_$eq', 'org$apache$spark$ml$param$shared$HasMaxIter$_setter_$maxIter_$eq', 'org$apache$spark$ml$param$shared$HasPredictionCol$_setter_$predictionCol_$eq', 'org$apache$spark$ml$param$shared$HasRegParam$_setter_$regParam_$eq', 'org$apache$spark$ml$param$shared$HasSeed$_setter_$seed_$eq', 'org$apache$spark$ml$recommendation$ALSModelParams$_setter_$checkedCast_$eq', 'org$apache$spark$ml$recommendation$ALSModelParams$_setter_$coldStartStrategy_$eq', 'org$apache$spark$ml$recommendation$ALSModelParams$_setter_$itemCol_$eq', 'org$apache$spark$ml$recommendation$ALSModelParams$_setter_$userCol_$eq', 'org$apache$spark$ml$recommendation$ALSParams$_setter_$alpha_$eq', 'org$apache$spark$ml$recommendation$ALSParams$_setter_$finalStorageLevel_$eq', 'org$apache$spark$ml$recommendation$ALSParams$_setter_$implicitPrefs_$eq', 'org$apache$spark$ml$recommendation$ALSParams$_setter_$intermediateStorageLevel_$eq', 'org$apache$spark$ml$recommendation$ALSParams$_setter_$nonnegative_$eq', 'org$apache$spark$ml$recommendation$ALSParams$_setter_$numItemBlocks_$eq', 'org$apache$spark$ml$recommendation$ALSParams$_setter_$numUserBlocks_$eq', 'org$apache$spark$ml$recommendation$ALSParams$_setter_$rank_$eq', 'org$apache$spark$ml$recommendation$ALSParams$_setter_$ratingCol_$eq', 'params', 'predictionCol', 'rank', 'ratingCol', 'read', 'regParam', 'save', 'seed', 'set', 'setAlpha', 'setCheckpointInterval', 'setColdStartStrategy', 'setDefault', 'setFinalStorageLevel', 'setImplicitPrefs', 'setIntermediateStorageLevel', 'setItemCol', 'setMaxIter', 'setNonnegative', 'setNumBlocks', 'setNumItemBlocks', 'setNumUserBlocks', 'setPredictionCol', 'setRank', 'setRatingCol', 'setRegParam', 'setSeed', 'setUserCol', 'toString', 'train', 'train$default$10', 'train$default$11', 'train$default$12', 'train$default$13', 'train$default$2', 'train$default$3', 'train$default$4', 'train$default$5', 'train$default$6', 'train$default$7', 'train$default$8', 'train$default$9', 'transformSchema', 'uid', 'userCol', 'validateAndTransformSchema', 'wait', 'write']\n"
     ]
    }
   ],
   "source": [
    "print(dir(bet_model_als._java_obj.parent()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------+----------+--------------+------------+-----------+\n",
      "|user_id|profile_id|rating| label|log_rating|log_rating_069|norm_log_069| prediction|\n",
      "+-------+----------+------+------+----------+--------------+------------+-----------+\n",
      "|  27657|       496|     6|0.5556|    2.7918|        2.4818|      0.7782| 0.44840348|\n",
      "| 133795|       496|    10|   1.0|    3.3026|        2.9926|         1.0| 0.75698173|\n",
      "|  83542|       496|    10|   1.0|    3.3026|        2.9926|         1.0| 0.98405075|\n",
      "|  37913|       496|     5|0.4444|    2.6094|        2.2994|      0.6989| 0.44741356|\n",
      "|   4100|       496|    10|   1.0|    3.3026|        2.9926|         1.0| 0.55546737|\n",
      "|  29441|       496|     6|0.5556|    2.7918|        2.4818|      0.7782| 0.68115723|\n",
      "| 129820|       496|     6|0.5556|    2.7918|        2.4818|      0.7782|  0.7318838|\n",
      "|  93285|      1238|     9|0.8889|    3.1972|        2.8872|      0.9542|   0.733231|\n",
      "|  25280|      1238|     1|   0.0|       1.0|          0.69|         0.0| 0.10579233|\n",
      "| 129612|      1238|     1|   0.0|       1.0|          0.69|         0.0| 0.14811707|\n",
      "|  35899|      1238|     1|   0.0|       1.0|          0.69|         0.0| 0.58082825|\n",
      "|  11072|      1238|     2|0.1111|    1.6931|        1.3831|       0.301| 0.21045786|\n",
      "|  85372|      1238|     6|0.5556|    2.7918|        2.4818|      0.7782| 0.44728976|\n",
      "|  87914|      1238|     1|   0.0|       1.0|          0.69|         0.0|0.029912265|\n",
      "| 127227|      1238|     4|0.3333|    2.3863|        2.0763|      0.6021| 0.30192673|\n",
      "|  61807|      1238|     9|0.8889|    3.1972|        2.8872|      0.9542| 0.24203245|\n",
      "|  20477|      1238|     9|0.8889|    3.1972|        2.8872|      0.9542| 0.58478147|\n",
      "|  25545|      1238|     2|0.1111|    1.6931|        1.3831|       0.301| 0.41841176|\n",
      "|  85632|      1238|     7|0.6667|    2.9459|        2.6359|      0.8451|  0.5835989|\n",
      "| 109161|      1238|     4|0.3333|    2.3863|        2.0763|      0.6021|0.080387786|\n",
      "| 133363|      1238|    10|   1.0|    3.3026|        2.9926|         1.0|  0.5422727|\n",
      "| 104374|      1238|     6|0.5556|    2.7918|        2.4818|      0.7782| 0.25068662|\n",
      "|  98413|      1238|     7|0.6667|    2.9459|        2.6359|      0.8451| 0.43855983|\n",
      "|  10201|      1238|     6|0.5556|    2.7918|        2.4818|      0.7782|0.035299182|\n",
      "|  70741|      1238|     4|0.3333|    2.3863|        2.0763|      0.6021|  0.4785083|\n",
      "+-------+----------+------+------+----------+--------------+------------+-----------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = bet_model_als.transform(test)\n",
    "predictions.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
