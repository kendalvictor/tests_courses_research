{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\")\\\n",
    ".option(\"header\", \"true\") \\\n",
    ".option(\"inferSchema\", \"false\") \\\n",
    ".load(ruta_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "access_id = \"-------\"\n",
    "access_key = \"-------\"\n",
    "aws_region = \"us-east-1\"\n",
    "bucket_name = \"ue1stgdesaas3dtl001\"\n",
    "file_starting_txt = \"starting.txt\"\n",
    "file_loading_txt = \"loading.txt\"\n",
    "file_processing_txt = \"processing.txt\"\n",
    "file_completing_txt = \"completing.txt\"\n",
    "file_completing_parquet_txt = \"completing_parquet.txt\"\n",
    "file_structure_pending_txt = \"structure_pending.txt\"\n",
    "file_structure_processing_txt = \"structure_processing.txt\"\n",
    "file_empty = \"\"\n",
    "\n",
    "type_alert = [\"INFO\", \"WARNING\", \"SUCCESS\"]\n",
    "\n",
    "folder_project = \"BDAX\"\n",
    "folder_schema = \"EMILY\"\n",
    "folder_table = str(args['TABLE_NAME'])\n",
    "folder_process_name = \"PROCESS\"\n",
    "folder_structure_name = \"STRUCTURE\"\n",
    "folder_sequence_name = \"SEQUENCE\"\n",
    "folder_detail_name = \"DETAIL\"\n",
    "folder_meta_store_name = \"META_STORE\"\n",
    "folder_lzn_name = \"LZN\"\n",
    "\n",
    "filter_schema_name = \"{0}/{1}/\".format(folder_project, folder_schema)\n",
    "filter_table_name = \"{0}{1}/\".format(filter_schema_name, folder_table)\n",
    "filter_process_name = \"{0}{1}/\".format(filter_table_name, folder_process_name)\n",
    "filter_structure_name = \"{0}{1}/\".format(filter_table_name, folder_structure_name)\n",
    "filter_sequence_name = \"{0}{1}/\".format(filter_table_name, folder_sequence_name)\n",
    "filter_detail_name = \"{0}{1}/\".format(filter_table_name, folder_detail_name)\n",
    "filter_meta_store_name = \"{0}{1}/\".format(filter_table_name, folder_meta_store_name)\n",
    "filter_lzn_name = \"{0}{1}/\".format(filter_table_name, folder_lzn_name)\n",
    "\n",
    "create_file_table_processing = \"{0}{1}\".format(filter_table_name, file_processing_txt)\n",
    "\n",
    "create_file_process_processing = \"{0}{1}\".format(filter_process_name, file_processing_txt)\n",
    "create_file_process_starting = \"{0}{1}\".format(filter_process_name, file_starting_txt)\n",
    "create_file_process_loading = \"{0}{1}\".format(filter_process_name, file_loading_txt)\n",
    "create_file_process_completing = \"{0}{1}\".format(filter_process_name, file_completing_txt)\n",
    "\n",
    "create_file_structure_pending = \"{0}{1}\".format(filter_structure_name, file_structure_pending_txt)\n",
    "create_file_structure_processing = \"{0}{1}\".format(filter_structure_name, file_structure_processing_txt)\n",
    "\n",
    "dfc = spark.read.parquet('s3://ue1stgdesaas3dtl001/BDAX/APP_DMS/EST_RAMO/LZN/01/01.parquet')\n",
    "#dfd = spark.read.parquet('s3://ue1stgdesaas3dtl001/BDAX/DIMENSIONES/DIM_DATOPARTICULAR/DIM_DATOPARTICULAR/01.parquet')\n",
    "#dfd = spark.read.parquet('s3://ue1stgdesaas3dtl001/BDAX/ACSELX/DATO_PARTICULAR/WORKING/TMP_TEMPORAL/bloque=04')\n",
    "\n",
    "print('Lectura de parquet')\n",
    "#print(dftr.first())\n",
    "#print(dfd.first())\n",
    "dfc.printSchema()\n",
    "#dfd.printSchema()\n",
    "#dfd.printSchema()\n",
    "#dfagen.registerTempTable(\"AGENCIAS\")\n",
    "\n",
    "\n",
    "job.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
