{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aggregation', 'DataFrame', 'Index', 'Series', '_Frame', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'absolute_import', 'accessor', 'categorical', 'compute', 'concat', 'core', 'demo', 'division', 'from_array', 'from_bcolz', 'from_dask_array', 'from_delayed', 'from_pandas', 'get_dummies', 'groupby', 'hashing', 'io', 'isna', 'map_partitions', 'melt', 'merge', 'methods', 'multi', 'optimize', 'pivot_table', 'print_function', 'read_csv', 'read_hdf', 'read_orc', 'read_parquet', 'read_sql_table', 'read_table', 'repartition', 'reshape', 'rolling', 'shuffle', 'to_bag', 'to_csv', 'to_datetime', 'to_delayed', 'to_hdf', 'to_parquet', 'to_records', 'to_timedelta', 'utils']\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "bucket_name = \"aatempjson2\"\n",
    "hour = 19\n",
    "path_json_spark = 's3://{}/{}/{}/{}/{}/*'.format(\n",
    "    bucket_name, '2019', '04', '08', hour\n",
    ")\n",
    "\n",
    "print(dir(dd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_csv in module dask.dataframe.io.csv:\n",
      "\n",
      "read_csv(urlpath, blocksize=64000000, collection=True, lineterminator=None, compression=None, sample=256000, enforce=False, assume_missing=False, storage_options=None, **kwargs)\n",
      "    Read CSV files into a Dask.DataFrame\n",
      "    \n",
      "    This parallelizes the :func:`pandas.read_csv` function in the following ways:\n",
      "    \n",
      "    - It supports loading many files at once using globstrings:\n",
      "    \n",
      "        >>> df = dd.read_csv('myfiles.*.csv')  # doctest: +SKIP\n",
      "    \n",
      "    - In some cases it can break up large files:\n",
      "    \n",
      "        >>> df = dd.read_csv('largefile.csv', blocksize=25e6)  # 25MB chunks  # doctest: +SKIP\n",
      "    \n",
      "    - It can read CSV files from external resources (e.g. S3, HDFS) by\n",
      "      providing a URL:\n",
      "    \n",
      "        >>> df = dd.read_csv('s3://bucket/myfiles.*.csv')  # doctest: +SKIP\n",
      "        >>> df = dd.read_csv('hdfs:///myfiles.*.csv')  # doctest: +SKIP\n",
      "        >>> df = dd.read_csv('hdfs://namenode.example.com/myfiles.*.csv')  # doctest: +SKIP\n",
      "    \n",
      "    Internally ``dd.read_csv`` uses :func:`pandas.read_csv` and supports many of the\n",
      "    same keyword arguments with the same performance guarantees. See the docstring\n",
      "    for :func:`pandas.read_csv` for more information on available keyword arguments.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    urlpath : string or list\n",
      "        Absolute or relative filepath(s). Prefix with a protocol like ``s3://``\n",
      "        to read from alternative filesystems. To read from multiple files you\n",
      "        can pass a globstring or a list of paths, with the caveat that they\n",
      "        must all have the same protocol.\n",
      "    blocksize : int or None, optional\n",
      "        Number of bytes by which to cut up larger files. Default value is\n",
      "        computed based on available physical memory and the number of cores.\n",
      "        If ``None``, use a single block for each file.\n",
      "    collection : boolean, optional\n",
      "        Return a dask.dataframe if True or list of dask.delayed objects if False\n",
      "    sample : int, optional\n",
      "        Number of bytes to use when determining dtypes\n",
      "    assume_missing : bool, optional\n",
      "        If True, all integer columns that aren't specified in ``dtype`` are assumed\n",
      "        to contain missing values, and are converted to floats. Default is False.\n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc.\n",
      "    **kwargs\n",
      "        Extra keyword arguments to forward to :func:`pandas.read_csv`.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Dask dataframe tries to infer the ``dtype`` of each column by reading a sample\n",
      "    from the start of the file (or of the first file if it's a glob). Usually this\n",
      "    works fine, but if the ``dtype`` is different later in the file (or in other\n",
      "    files) this can cause issues. For example, if all the rows in the sample had\n",
      "    integer dtypes, but later on there was a ``NaN``, then this would error at\n",
      "    compute time. To fix this, you have a few options:\n",
      "    \n",
      "    - Provide explicit dtypes for the offending columns using the ``dtype``\n",
      "      keyword. This is the recommended solution.\n",
      "    \n",
      "    - Use the ``assume_missing`` keyword to assume that all columns inferred as\n",
      "      integers contain missing values, and convert them to floats.\n",
      "    \n",
      "    - Increase the size of the sample using the ``sample`` keyword.\n",
      "    \n",
      "    It should also be noted that this function may fail if a CSV file\n",
      "    includes quoted strings that contain the line terminator. To get around this\n",
      "    you can specify ``blocksize=None`` to not split files into multiple partitions,\n",
      "    at the cost of reduced parallelism.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(dd.read_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_HTML_FMT', '__abs__', '__add__', '__and__', '__array__', '__array_ufunc__', '__array_wrap__', '__bool__', '__class__', '__complex__', '__dask_graph__', '__dask_keys__', '__dask_optimize__', '__dask_postcompute__', '__dask_postpersist__', '__dask_scheduler__', '__dask_tokenize__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__int__', '__invert__', '__le__', '__len__', '__long__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rand__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '__xor__', '_args', '_bind_comparison_method', '_bind_operator', '_bind_operator_method', '_constructor', '_contains_index_name', '_cum_agg', '_elemwise', '_get_binary_operator', '_get_numeric_data', '_get_unary_operator', '_is_column_label_reference', '_is_index_level_reference', '_meta', '_meta_nonempty', '_name', '_partition_type', '_reduction_agg', '_repr_data', '_repr_divisions', '_repr_html_', '_scalarfunc', '_select_columns_or_index', '_token_prefix', '_validate_axis', 'abs', 'add', 'align', 'all', 'any', 'append', 'apply', 'applymap', 'assign', 'astype', 'bfill', 'categorize', 'clear_divisions', 'clip', 'clip_lower', 'clip_upper', 'columns', 'combine', 'combine_first', 'compute', 'copy', 'corr', 'count', 'cov', 'cummax', 'cummin', 'cumprod', 'cumsum', 'dask', 'describe', 'diff', 'div', 'divisions', 'drop', 'drop_duplicates', 'dropna', 'dtypes', 'eq', 'eval', 'ffill', 'fillna', 'first', 'floordiv', 'ge', 'get_dtype_counts', 'get_ftype_counts', 'get_partition', 'groupby', 'gt', 'head', 'idxmax', 'idxmin', 'index', 'info', 'isin', 'isna', 'isnull', 'iterrows', 'itertuples', 'join', 'known_divisions', 'last', 'le', 'loc', 'lt', 'map_overlap', 'map_partitions', 'mask', 'max', 'mean', 'memory_usage', 'merge', 'min', 'mod', 'mul', 'ndim', 'ne', 'nlargest', 'notnull', 'npartitions', 'nsmallest', 'nunique_approx', 'persist', 'pipe', 'pivot_table', 'pow', 'prod', 'quantile', 'query', 'radd', 'random_split', 'rdiv', 'reduction', 'rename', 'repartition', 'resample', 'reset_index', 'rfloordiv', 'rmod', 'rmul', 'rolling', 'round', 'rpow', 'rsub', 'rtruediv', 'sample', 'select_dtypes', 'sem', 'set_index', 'shift', 'size', 'squeeze', 'std', 'sub', 'sum', 'tail', 'to_bag', 'to_csv', 'to_delayed', 'to_hdf', 'to_html', 'to_parquet', 'to_records', 'to_string', 'to_timestamp', 'truediv', 'values', 'var', 'visualize', 'where']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmg/anaconda3/lib/python3.6/site-packages/dask/dataframe/core.py:4206: UserWarning: Insufficient elements for `head`. 5 elements requested, only 2 elements available. Try passing larger `npartitions` to `head`.\n",
      "  warnings.warn(msg.format(n, len(r)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"project\":\"ojo_dev\",\"event\":\"receive\",\"date\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"project\":\"ojo_dev\",\"event\":\"receive\",\"date\":...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  {\"project\":\"ojo_dev\",\"event\":\"receive\",\"date\":...\n",
       "0  {\"project\":\"ojo_dev\",\"event\":\"receive\",\"date\":..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dd.read_csv(path_json_spark, sep='@@', engine='python', header=None)\n",
    "print(dir(df))\n",
    "df.head(npartitions=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"project\":\"ojo_dev\",\"event\":\"receive\",\"date\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"project\":\"ojo_dev\",\"event\":\"receive\",\"date\":...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  {\"project\":\"ojo_dev\",\"event\":\"receive\",\"date\":...\n",
       "0  {\"project\":\"ojo_dev\",\"event\":\"receive\",\"date\":..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.compute()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<method-wrapper '__str__' of method object at 0x7fc90e513c88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
