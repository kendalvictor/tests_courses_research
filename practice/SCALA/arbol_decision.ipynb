{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.60:4040\n",
       "SparkContext available as 'sc' (version = 3.0.0-preview2, master = local[*], app id = local-1592146840056)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 36.49453783035278 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.tree.DecisionTree\n",
       "import org.apache.spark.mllib.tree.model.DecisionTreeModel\n",
       "import org.apache.spark.mllib.util.MLUtils\n",
       "import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "import org.apache.spark.mllib.regression.LabeledPoint\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.model.DecisionTreeModel\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.mllib.regression.LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[86] at map at MLUtils.scala:86\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = MLUtils.loadLibSVMFile(sc, \"a1a.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "splits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[89] at randomSplit at <console>:48, MapPartitionsRDD[90] at randomSplit at <console>:48)\n",
       "trainingData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[89] at randomSplit at <console>:48\n",
       "testData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[90] at randomSplit at <console>:48\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val splits = data.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, testData) = (splits(0), splits(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[89] at randomSplit at <console>:48\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[90] at randomSplit at <console>:48\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numClasses: Int = 2\n",
       "categoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map()\n",
       "impurity: String = gini\n",
       "maxDepth: Int = 5\n",
       "maxBins: Int = 32\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numClasses = 2\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val impurity = \"gini\"\n",
    "val maxDepth = 5\n",
    "val maxBins = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel classifier of depth 1 with 3 nodes\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,\n",
    "  impurity, maxDepth, maxBins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelAndPreds: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[103] at map at <console>:45\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelAndPreds = testData.map { point =>\n",
    "  val prediction = model.predict(point.features)\n",
    "  (point.label, prediction)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.03125\n",
      "Learned classification tree model:\n",
      " DecisionTreeModel classifier of depth 1 with 3 nodes\n",
      "  If (feature 434 <= 79.5)\n",
      "   Predict: 0.0\n",
      "  Else (feature 434 > 79.5)\n",
      "   Predict: 1.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testErr: Double = 0.03125\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testErr = labelAndPreds.filter(r => r._1 != r._2).count().toDouble / testData.count()\n",
    "println(s\"Test Error = $testErr\")\n",
    "println(s\"Learned classification tree model:\\n ${model.toDebugString}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sameModel: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel classifier of depth 1 with 3 nodes\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Save and load model\n",
    "model.save(sc, \"target/tmp/myDecisionTreeClassificationModel\")\n",
    "val sameModel = DecisionTreeModel.load(sc, \"target/tmp/myDecisionTreeClassificationModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
