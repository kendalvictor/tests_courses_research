{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.60:4040\n",
       "SparkContext available as 'sc' (version = 3.0.0-preview2, master = local[*], app id = local-1593940868680)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}\n",
       "import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "import org.apache.spark.mllib.tree.RandomForest\n",
       "import org.apache.spark.mllib.tree.model.RandomForestModel\n",
       "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
       "import org.apache.spark.mllib.regression.LabeledPoint\n",
       "import org.apache.spark.mllib.util.MLUtils\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}\n",
    "import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.mllib.tree.RandomForest\n",
    "import org.apache.spark.mllib.tree.model.RandomForestModel\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.util.MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:86\n",
       "res0: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:86\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = MLUtils.loadLibSVMFile(sc, \"sample_libsvm_data.txt\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.mllib.regression.LabeledPoint = (0.0,(692,[127,128,129,130,131,154,155,156,157,158,159,181,182,183,184,185,186,187,188,189,207,208,209,210,211,212,213,214,215,216,217,235,236,237,238,239,240,241,242,243,244,245,262,263,264,265,266,267,268,269,270,271,272,273,289,290,291,292,293,294,295,296,297,300,301,302,316,317,318,319,320,321,328,329,330,343,344,345,346,347,348,349,356,357,358,371,372,373,374,384,385,386,399,400,401,412,413,414,426,427,428,429,440,441,442,454,455,456,457,466,467,468,469,470,482,483,484,493,494,495,496,497,510,511,512,520,521,522,523,538,539,540,547,548,549,550,566,567,568,569,570,571,572,573,574,575,576,577,578,594,595,596,597,598,599,600,601,602,603,604,622,623,624,625,626,627,628,629,630,651,652,653,654,655,656,657],[51.0,159.0,253.0,159.0,50...\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.mllib.regression.LabeledPoint = (1.0,(692,[158,159,160,161,185,186,187,188,189,213,214,215,216,217,240,241,242,243,244,245,267,268,269,270,271,295,296,297,298,322,323,324,325,326,349,350,351,352,353,377,378,379,380,381,404,405,406,407,408,431,432,433,434,435,459,460,461,462,463,486,487,488,489,490,514,515,516,517,518,542,543,544,545,569,570,571,572,573,596,597,598,599,600,601,624,625,626,627,652,653,654,655,680,681,682,683],[124.0,253.0,255.0,63.0,96.0,244.0,251.0,253.0,62.0,127.0,251.0,251.0,253.0,62.0,68.0,236.0,251.0,211.0,31.0,8.0,60.0,228.0,251.0,251.0,94.0,155.0,253.0,253.0,189.0,20.0,253.0,251.0,235.0,66.0,32.0,205.0,253.0,251.0,126.0,104.0,251.0,253.0,184.0,15.0,80.0,240.0,251.0,193.0,23.0,32.0,253.0,253.0,253.0,159.0,151.0,251.0,251.0,251.0,39.0,48.0,221....\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "splits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at <console>:34, MapPartitionsRDD[8] at randomSplit at <console>:34)\n",
       "trainingData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at <console>:34\n",
       "testData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at <console>:34\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val splits = data.randomSplit(Array(0.6, 0.4), seed = 111)\n",
    "\n",
    "val (trainingData, testData) = (splits(0), splits(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Long = 63\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Long = 37\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Long = 100\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENTRENAMIENTO DE MULTIPLES MODELOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### /////////////////////////////////////////////  Regesión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_logistic: org.apache.spark.mllib.classification.LogisticRegressionModel = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 692, numClasses = 2, threshold = 0.5\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model_logistic = new LogisticRegressionWithLBFGS()\n",
    "  .setNumClasses(2)\n",
    "  .run(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionAndLabels_logistic: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[65] at map at <console>:36\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionAndLabels_logistic = testData.map { case LabeledPoint(label, features) =>\n",
    "  val prediction = model_logistic.predict(features)\n",
    "  (prediction, label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_logistic: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@6668e4b7\n",
       "auROC_logistic: Double = 0.9782608695652174\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics_logistic = new BinaryClassificationMetrics(predictionAndLabels_logistic)\n",
    "val auROC_logistic = metrics_logistic.areaUnderROC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ///////////////////////////////////////////// SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numIterations: Int = 100\n",
       "model_svm: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 692, numClasses = 2, threshold = 0.0\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numIterations = 100\n",
    "val model_svm = SVMWithSGD.train(\n",
    "    trainingData, numIterations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scoreAndLabels_svm: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[283] at map at <console>:36\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scoreAndLabels_svm = testData.map { point =>\n",
    "  val score = model_svm.predict(point.features)\n",
    "  (score, point.label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_svm: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@461a5372\n",
       "auROC_svm: Double = 1.0\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics_svm = new BinaryClassificationMetrics(scoreAndLabels_svm)\n",
    "val auROC_svm = metrics_svm.areaUnderROC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ///////////////////////////////////////////// RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numClasses: Int = 2\n",
       "categoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map()\n",
       "numTrees: Int = 30\n",
       "featureSubsetStrategy: String = auto\n",
       "impurity: String = gini\n",
       "maxDepth: Int = 4\n",
       "maxBins: Int = 32\n",
       "model_forest: org.apache.spark.mllib.tree.model.RandomForestModel =\n",
       "TreeEnsembleModel classifier with 30 trees\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numClasses = 2\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val numTrees = 30 // Use more in practice.\n",
    "val featureSubsetStrategy = \"auto\" // Let the algorithm choose.\n",
    "val impurity = \"gini\"\n",
    "val maxDepth = 4\n",
    "val maxBins = 32\n",
    "\n",
    "val model_forest = RandomForest.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,\n",
    "  numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scoreAndLabels_forest: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[316] at map at <console>:36\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scoreAndLabels_forest = testData.map { point =>\n",
    "  val prediction_forest = model_forest.predict(point.features)\n",
    "  (prediction_forest, point.label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_forest: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@6272f352\n",
       "auROC_forest: Double = 1.0\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics_forest = new BinaryClassificationMetrics(scoreAndLabels_forest)\n",
    "val auROC_forest = metrics_forest.areaUnderROC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ///////////////////////////////////////////// Persistencia de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path_save: String = target/model_forest2\n",
       "model_forest_load: org.apache.spark.mllib.tree.model.RandomForestModel =\n",
       "TreeEnsembleModel classifier with 30 trees\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path_save = \"target/model_forest2\"\n",
    "\n",
    "model_forest.save(sc, path_save)\n",
    "\n",
    "val model_forest_load = RandomForestModel.load(\n",
    "  sc, path_save\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scoreAndLabels_forest_load: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[355] at map at <console>:36\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scoreAndLabels_forest_load = testData.map { point =>\n",
    "  val prediction_forest_load = model_forest_load.predict(point.features)\n",
    "  (prediction_forest_load, point.label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_forest_load: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@7b2e75f0\n",
       "auROC_forest_load: Double = 1.0\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics_forest_load = new BinaryClassificationMetrics(scoreAndLabels_forest_load)\n",
    "val auROC_forest_load = metrics_forest_load.areaUnderROC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
